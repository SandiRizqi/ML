{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31cb2a99-dcc2-4e04-b9cd-84fd8fa1f1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcd73d46-2ff4-48ec-81ab-8e6e5130dcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scaler object\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1ac8f32-f99c-4450-8737-c507e1d8107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('./datasets/Band_dataset_GBSM_AAPA - Sheet1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c11f10c4-2d22-412a-8101-4d226465d1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMP</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>band1</th>\n",
       "      <th>band2</th>\n",
       "      <th>band3</th>\n",
       "      <th>band4</th>\n",
       "      <th>ndvi</th>\n",
       "      <th>N_Actual</th>\n",
       "      <th>P_Actual</th>\n",
       "      <th>K_Actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPA</td>\n",
       "      <td>7391</td>\n",
       "      <td>766</td>\n",
       "      <td>706</td>\n",
       "      <td>629</td>\n",
       "      <td>388</td>\n",
       "      <td>2649</td>\n",
       "      <td>0.744485</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPA</td>\n",
       "      <td>7381</td>\n",
       "      <td>1065</td>\n",
       "      <td>679</td>\n",
       "      <td>535</td>\n",
       "      <td>323</td>\n",
       "      <td>2819</td>\n",
       "      <td>0.794398</td>\n",
       "      <td>3.03</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPA</td>\n",
       "      <td>7308</td>\n",
       "      <td>1074</td>\n",
       "      <td>670</td>\n",
       "      <td>523</td>\n",
       "      <td>315</td>\n",
       "      <td>2583</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPA</td>\n",
       "      <td>7255</td>\n",
       "      <td>1184</td>\n",
       "      <td>695</td>\n",
       "      <td>550</td>\n",
       "      <td>342</td>\n",
       "      <td>2602</td>\n",
       "      <td>0.767663</td>\n",
       "      <td>2.86</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPA</td>\n",
       "      <td>7082</td>\n",
       "      <td>839</td>\n",
       "      <td>683</td>\n",
       "      <td>564</td>\n",
       "      <td>370</td>\n",
       "      <td>2516</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>2.95</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>GBSM</td>\n",
       "      <td>35</td>\n",
       "      <td>531</td>\n",
       "      <td>243</td>\n",
       "      <td>408</td>\n",
       "      <td>278</td>\n",
       "      <td>3287</td>\n",
       "      <td>0.862323</td>\n",
       "      <td>2.24</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>GBSM</td>\n",
       "      <td>34</td>\n",
       "      <td>633</td>\n",
       "      <td>236</td>\n",
       "      <td>429</td>\n",
       "      <td>275</td>\n",
       "      <td>3356</td>\n",
       "      <td>0.868597</td>\n",
       "      <td>2.94</td>\n",
       "      <td>0.200</td>\n",
       "      <td>1.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>GBSM</td>\n",
       "      <td>33</td>\n",
       "      <td>531</td>\n",
       "      <td>245</td>\n",
       "      <td>403</td>\n",
       "      <td>286</td>\n",
       "      <td>3314</td>\n",
       "      <td>0.862321</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.160</td>\n",
       "      <td>1.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>GBSM</td>\n",
       "      <td>33</td>\n",
       "      <td>545</td>\n",
       "      <td>226</td>\n",
       "      <td>383</td>\n",
       "      <td>284</td>\n",
       "      <td>3263</td>\n",
       "      <td>0.870450</td>\n",
       "      <td>2.91</td>\n",
       "      <td>0.170</td>\n",
       "      <td>1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>GBSM</td>\n",
       "      <td>31</td>\n",
       "      <td>544</td>\n",
       "      <td>226</td>\n",
       "      <td>395</td>\n",
       "      <td>291</td>\n",
       "      <td>3356</td>\n",
       "      <td>0.873814</td>\n",
       "      <td>2.59</td>\n",
       "      <td>0.170</td>\n",
       "      <td>1.29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>710 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     COMP     x     y  band1  band2  band3  band4      ndvi  N_Actual  \\\n",
       "0    AAPA  7391   766    706    629    388   2649  0.744485      2.07   \n",
       "1    AAPA  7381  1065    679    535    323   2819  0.794398      3.03   \n",
       "2    AAPA  7308  1074    670    523    315   2583  0.782609      3.06   \n",
       "3    AAPA  7255  1184    695    550    342   2602  0.767663      2.86   \n",
       "4    AAPA  7082   839    683    564    370   2516  0.743590      2.95   \n",
       "..    ...   ...   ...    ...    ...    ...    ...       ...       ...   \n",
       "705  GBSM    35   531    243    408    278   3287  0.862323      2.24   \n",
       "706  GBSM    34   633    236    429    275   3356  0.868597      2.94   \n",
       "707  GBSM    33   531    245    403    286   3314  0.862321      2.40   \n",
       "708  GBSM    33   545    226    383    284   3263  0.870450      2.91   \n",
       "709  GBSM    31   544    226    395    291   3356  0.873814      2.59   \n",
       "\n",
       "     P_Actual  K_Actual  \n",
       "0       0.165      0.69  \n",
       "1       0.187      0.51  \n",
       "2       0.178      0.55  \n",
       "3       0.181      0.56  \n",
       "4       0.187      0.58  \n",
       "..        ...       ...  \n",
       "705     0.170      0.96  \n",
       "706     0.200      1.28  \n",
       "707     0.160      1.21  \n",
       "708     0.170      1.20  \n",
       "709     0.170      1.29  \n",
       "\n",
       "[710 rows x 11 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cabc26a-45bd-4827-bbaf-b5e0c36ec5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[['band1', 'band2', 'band3', 'band4', 'ndvi', 'N_Actual', 'P_Actual', 'K_Actual']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32a9d11c-281a-4516-a30f-79cc89a63abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9z/n3j81_ls7r5_5wm38p_06kbw0000gn/T/ipykernel_24075/3663055858.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[bands] = scaler.fit_transform(dataset[bands])\n"
     ]
    }
   ],
   "source": [
    "bands = ['band1', 'band2', 'band3', 'band4']\n",
    "\n",
    "# Normalize the selected bands\n",
    "dataset[bands] = scaler.fit_transform(dataset[bands])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2797a4b-6b0a-4f1b-88e2-7ff87e4947cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>band1</th>\n",
       "      <th>band2</th>\n",
       "      <th>band3</th>\n",
       "      <th>band4</th>\n",
       "      <th>ndvi</th>\n",
       "      <th>N_Actual</th>\n",
       "      <th>P_Actual</th>\n",
       "      <th>K_Actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.700483</td>\n",
       "      <td>0.396414</td>\n",
       "      <td>0.605311</td>\n",
       "      <td>0.744485</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.873684</td>\n",
       "      <td>0.549114</td>\n",
       "      <td>0.266932</td>\n",
       "      <td>0.656608</td>\n",
       "      <td>0.794398</td>\n",
       "      <td>3.03</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.860150</td>\n",
       "      <td>0.529791</td>\n",
       "      <td>0.250996</td>\n",
       "      <td>0.585395</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.897744</td>\n",
       "      <td>0.573269</td>\n",
       "      <td>0.304781</td>\n",
       "      <td>0.591129</td>\n",
       "      <td>0.767663</td>\n",
       "      <td>2.86</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.879699</td>\n",
       "      <td>0.595813</td>\n",
       "      <td>0.360558</td>\n",
       "      <td>0.565178</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>2.95</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>0.218045</td>\n",
       "      <td>0.344605</td>\n",
       "      <td>0.177291</td>\n",
       "      <td>0.797827</td>\n",
       "      <td>0.862323</td>\n",
       "      <td>2.24</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>0.207519</td>\n",
       "      <td>0.378422</td>\n",
       "      <td>0.171315</td>\n",
       "      <td>0.818648</td>\n",
       "      <td>0.868597</td>\n",
       "      <td>2.94</td>\n",
       "      <td>0.200</td>\n",
       "      <td>1.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>0.221053</td>\n",
       "      <td>0.336554</td>\n",
       "      <td>0.193227</td>\n",
       "      <td>0.805975</td>\n",
       "      <td>0.862321</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.160</td>\n",
       "      <td>1.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>0.192481</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.189243</td>\n",
       "      <td>0.790585</td>\n",
       "      <td>0.870450</td>\n",
       "      <td>2.91</td>\n",
       "      <td>0.170</td>\n",
       "      <td>1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>0.192481</td>\n",
       "      <td>0.323671</td>\n",
       "      <td>0.203187</td>\n",
       "      <td>0.818648</td>\n",
       "      <td>0.873814</td>\n",
       "      <td>2.59</td>\n",
       "      <td>0.170</td>\n",
       "      <td>1.29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>710 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        band1     band2     band3     band4      ndvi  N_Actual  P_Actual  \\\n",
       "0    0.914286  0.700483  0.396414  0.605311  0.744485      2.07     0.165   \n",
       "1    0.873684  0.549114  0.266932  0.656608  0.794398      3.03     0.187   \n",
       "2    0.860150  0.529791  0.250996  0.585395  0.782609      3.06     0.178   \n",
       "3    0.897744  0.573269  0.304781  0.591129  0.767663      2.86     0.181   \n",
       "4    0.879699  0.595813  0.360558  0.565178  0.743590      2.95     0.187   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "705  0.218045  0.344605  0.177291  0.797827  0.862323      2.24     0.170   \n",
       "706  0.207519  0.378422  0.171315  0.818648  0.868597      2.94     0.200   \n",
       "707  0.221053  0.336554  0.193227  0.805975  0.862321      2.40     0.160   \n",
       "708  0.192481  0.304348  0.189243  0.790585  0.870450      2.91     0.170   \n",
       "709  0.192481  0.323671  0.203187  0.818648  0.873814      2.59     0.170   \n",
       "\n",
       "     K_Actual  \n",
       "0        0.69  \n",
       "1        0.51  \n",
       "2        0.55  \n",
       "3        0.56  \n",
       "4        0.58  \n",
       "..        ...  \n",
       "705      0.96  \n",
       "706      1.28  \n",
       "707      1.21  \n",
       "708      1.20  \n",
       "709      1.29  \n",
       "\n",
       "[710 rows x 8 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34fe1fc-e9a0-49b5-a43a-363df99e1b9c",
   "metadata": {},
   "source": [
    "BUILD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c59df00f-c3ec-42d0-b1bc-416f90c89d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff74cbd5-1060-4dcc-a0fc-a2fdeb94650c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.0.dev20250112\n",
      "MPS (Metal Performance Shaders) available: True\n",
      "Built with MPS: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"MPS (Metal Performance Shaders) available: {torch.backends.mps.is_available()}\")\n",
    "print(f\"Built with MPS: {torch.backends.mps.is_built()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e14606e7-cf5d-460c-9cf8-20314d47720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.features = dataframe[['band1', 'band2', 'band3', 'band4', 'ndvi']].values\n",
    "        self.targets = dataframe[['N_Actual', 'P_Actual', 'K_Actual']].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "        return x, y\n",
    "\n",
    "# Define the Neural Network\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5778b2ac-c4bf-47ef-83b9-14af946148c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe = pd.DataFrame(dataset)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_df = dataset.sample(frac=0.7, random_state=42)\n",
    "val_df = dataset.drop(train_df.index)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CustomDataset(train_df)\n",
    "val_dataset = CustomDataset(val_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5eb7852-55f9-418d-91bf-cae07651d610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length 497\n",
      "Val dataset length 213\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset length\", train_dataset.__len__())\n",
    "print(\"Val dataset length\", val_dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7732235f-f168-44d0-89ba-2c1ee5d34048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, Loss, and Optimizer\n",
    "input_size = 5  # Number of input features\n",
    "hidden_size = 64  # Hidden layer size\n",
    "output_size = 3  # Number of targets (N, P, K)\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04fd9bf5-bbcd-4fea-a93f-a2db151575a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Lists to store losses for plotting\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=50, save_path=\"best_model.pth\"):\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), f'./models/{save_path}')\n",
    "            print(f\"Best model saved with Val Loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d70f24e8-4bc4-47b6-82a8-04c106d2580a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Train Loss: 2.5109, Val Loss: 2.4595\n",
      "Best model saved with Val Loss: 2.4595\n",
      "Epoch [2/1000], Train Loss: 2.2510, Val Loss: 2.1961\n",
      "Best model saved with Val Loss: 2.1961\n",
      "Epoch [3/1000], Train Loss: 1.9920, Val Loss: 1.9368\n",
      "Best model saved with Val Loss: 1.9368\n",
      "Epoch [4/1000], Train Loss: 1.7525, Val Loss: 1.6767\n",
      "Best model saved with Val Loss: 1.6767\n",
      "Epoch [5/1000], Train Loss: 1.4862, Val Loss: 1.4094\n",
      "Best model saved with Val Loss: 1.4094\n",
      "Epoch [6/1000], Train Loss: 1.2216, Val Loss: 1.1440\n",
      "Best model saved with Val Loss: 1.1440\n",
      "Epoch [7/1000], Train Loss: 0.9687, Val Loss: 0.8861\n",
      "Best model saved with Val Loss: 0.8861\n",
      "Epoch [8/1000], Train Loss: 0.7354, Val Loss: 0.6515\n",
      "Best model saved with Val Loss: 0.6515\n",
      "Epoch [9/1000], Train Loss: 0.5106, Val Loss: 0.4513\n",
      "Best model saved with Val Loss: 0.4513\n",
      "Epoch [10/1000], Train Loss: 0.3486, Val Loss: 0.2990\n",
      "Best model saved with Val Loss: 0.2990\n",
      "Epoch [11/1000], Train Loss: 0.2157, Val Loss: 0.1906\n",
      "Best model saved with Val Loss: 0.1906\n",
      "Epoch [12/1000], Train Loss: 0.1399, Val Loss: 0.1254\n",
      "Best model saved with Val Loss: 0.1254\n",
      "Epoch [13/1000], Train Loss: 0.0937, Val Loss: 0.0890\n",
      "Best model saved with Val Loss: 0.0890\n",
      "Epoch [14/1000], Train Loss: 0.0720, Val Loss: 0.0714\n",
      "Best model saved with Val Loss: 0.0714\n",
      "Epoch [15/1000], Train Loss: 0.0627, Val Loss: 0.0627\n",
      "Best model saved with Val Loss: 0.0627\n",
      "Epoch [16/1000], Train Loss: 0.0565, Val Loss: 0.0591\n",
      "Best model saved with Val Loss: 0.0591\n",
      "Epoch [17/1000], Train Loss: 0.0548, Val Loss: 0.0578\n",
      "Best model saved with Val Loss: 0.0578\n",
      "Epoch [18/1000], Train Loss: 0.0556, Val Loss: 0.0569\n",
      "Best model saved with Val Loss: 0.0569\n",
      "Epoch [19/1000], Train Loss: 0.0565, Val Loss: 0.0561\n",
      "Best model saved with Val Loss: 0.0561\n",
      "Epoch [20/1000], Train Loss: 0.0523, Val Loss: 0.0555\n",
      "Best model saved with Val Loss: 0.0555\n",
      "Epoch [21/1000], Train Loss: 0.0528, Val Loss: 0.0552\n",
      "Best model saved with Val Loss: 0.0552\n",
      "Epoch [22/1000], Train Loss: 0.0526, Val Loss: 0.0551\n",
      "Best model saved with Val Loss: 0.0551\n",
      "Epoch [23/1000], Train Loss: 0.0515, Val Loss: 0.0547\n",
      "Best model saved with Val Loss: 0.0547\n",
      "Epoch [24/1000], Train Loss: 0.0519, Val Loss: 0.0546\n",
      "Best model saved with Val Loss: 0.0546\n",
      "Epoch [25/1000], Train Loss: 0.0501, Val Loss: 0.0545\n",
      "Best model saved with Val Loss: 0.0545\n",
      "Epoch [26/1000], Train Loss: 0.0501, Val Loss: 0.0541\n",
      "Best model saved with Val Loss: 0.0541\n",
      "Epoch [27/1000], Train Loss: 0.0510, Val Loss: 0.0539\n",
      "Best model saved with Val Loss: 0.0539\n",
      "Epoch [28/1000], Train Loss: 0.0516, Val Loss: 0.0539\n",
      "Best model saved with Val Loss: 0.0539\n",
      "Epoch [29/1000], Train Loss: 0.0503, Val Loss: 0.0532\n",
      "Best model saved with Val Loss: 0.0532\n",
      "Epoch [30/1000], Train Loss: 0.0512, Val Loss: 0.0533\n",
      "Epoch [31/1000], Train Loss: 0.0504, Val Loss: 0.0530\n",
      "Best model saved with Val Loss: 0.0530\n",
      "Epoch [32/1000], Train Loss: 0.0479, Val Loss: 0.0528\n",
      "Best model saved with Val Loss: 0.0528\n",
      "Epoch [33/1000], Train Loss: 0.0510, Val Loss: 0.0523\n",
      "Best model saved with Val Loss: 0.0523\n",
      "Epoch [34/1000], Train Loss: 0.0476, Val Loss: 0.0525\n",
      "Epoch [35/1000], Train Loss: 0.0484, Val Loss: 0.0520\n",
      "Best model saved with Val Loss: 0.0520\n",
      "Epoch [36/1000], Train Loss: 0.0484, Val Loss: 0.0519\n",
      "Best model saved with Val Loss: 0.0519\n",
      "Epoch [37/1000], Train Loss: 0.0469, Val Loss: 0.0515\n",
      "Best model saved with Val Loss: 0.0515\n",
      "Epoch [38/1000], Train Loss: 0.0465, Val Loss: 0.0514\n",
      "Best model saved with Val Loss: 0.0514\n",
      "Epoch [39/1000], Train Loss: 0.0497, Val Loss: 0.0513\n",
      "Best model saved with Val Loss: 0.0513\n",
      "Epoch [40/1000], Train Loss: 0.0473, Val Loss: 0.0510\n",
      "Best model saved with Val Loss: 0.0510\n",
      "Epoch [41/1000], Train Loss: 0.0488, Val Loss: 0.0510\n",
      "Best model saved with Val Loss: 0.0510\n",
      "Epoch [42/1000], Train Loss: 0.0471, Val Loss: 0.0512\n",
      "Epoch [43/1000], Train Loss: 0.0466, Val Loss: 0.0507\n",
      "Best model saved with Val Loss: 0.0507\n",
      "Epoch [44/1000], Train Loss: 0.0458, Val Loss: 0.0507\n",
      "Best model saved with Val Loss: 0.0507\n",
      "Epoch [45/1000], Train Loss: 0.0467, Val Loss: 0.0506\n",
      "Best model saved with Val Loss: 0.0506\n",
      "Epoch [46/1000], Train Loss: 0.0460, Val Loss: 0.0502\n",
      "Best model saved with Val Loss: 0.0502\n",
      "Epoch [47/1000], Train Loss: 0.0469, Val Loss: 0.0502\n",
      "Epoch [48/1000], Train Loss: 0.0452, Val Loss: 0.0499\n",
      "Best model saved with Val Loss: 0.0499\n",
      "Epoch [49/1000], Train Loss: 0.0453, Val Loss: 0.0501\n",
      "Epoch [50/1000], Train Loss: 0.0448, Val Loss: 0.0496\n",
      "Best model saved with Val Loss: 0.0496\n",
      "Epoch [51/1000], Train Loss: 0.0440, Val Loss: 0.0496\n",
      "Epoch [52/1000], Train Loss: 0.0448, Val Loss: 0.0495\n",
      "Best model saved with Val Loss: 0.0495\n",
      "Epoch [53/1000], Train Loss: 0.0443, Val Loss: 0.0493\n",
      "Best model saved with Val Loss: 0.0493\n",
      "Epoch [54/1000], Train Loss: 0.0454, Val Loss: 0.0492\n",
      "Best model saved with Val Loss: 0.0492\n",
      "Epoch [55/1000], Train Loss: 0.0447, Val Loss: 0.0489\n",
      "Best model saved with Val Loss: 0.0489\n",
      "Epoch [56/1000], Train Loss: 0.0440, Val Loss: 0.0490\n",
      "Epoch [57/1000], Train Loss: 0.0434, Val Loss: 0.0488\n",
      "Best model saved with Val Loss: 0.0488\n",
      "Epoch [58/1000], Train Loss: 0.0443, Val Loss: 0.0486\n",
      "Best model saved with Val Loss: 0.0486\n",
      "Epoch [59/1000], Train Loss: 0.0432, Val Loss: 0.0487\n",
      "Epoch [60/1000], Train Loss: 0.0431, Val Loss: 0.0485\n",
      "Best model saved with Val Loss: 0.0485\n",
      "Epoch [61/1000], Train Loss: 0.0426, Val Loss: 0.0483\n",
      "Best model saved with Val Loss: 0.0483\n",
      "Epoch [62/1000], Train Loss: 0.0433, Val Loss: 0.0482\n",
      "Best model saved with Val Loss: 0.0482\n",
      "Epoch [63/1000], Train Loss: 0.0436, Val Loss: 0.0483\n",
      "Epoch [64/1000], Train Loss: 0.0462, Val Loss: 0.0480\n",
      "Best model saved with Val Loss: 0.0480\n",
      "Epoch [65/1000], Train Loss: 0.0424, Val Loss: 0.0480\n",
      "Best model saved with Val Loss: 0.0480\n",
      "Epoch [66/1000], Train Loss: 0.0434, Val Loss: 0.0478\n",
      "Best model saved with Val Loss: 0.0478\n",
      "Epoch [67/1000], Train Loss: 0.0426, Val Loss: 0.0476\n",
      "Best model saved with Val Loss: 0.0476\n",
      "Epoch [68/1000], Train Loss: 0.0419, Val Loss: 0.0476\n",
      "Best model saved with Val Loss: 0.0476\n",
      "Epoch [69/1000], Train Loss: 0.0473, Val Loss: 0.0476\n",
      "Epoch [70/1000], Train Loss: 0.0420, Val Loss: 0.0473\n",
      "Best model saved with Val Loss: 0.0473\n",
      "Epoch [71/1000], Train Loss: 0.0425, Val Loss: 0.0475\n",
      "Epoch [72/1000], Train Loss: 0.0436, Val Loss: 0.0473\n",
      "Epoch [73/1000], Train Loss: 0.0432, Val Loss: 0.0475\n",
      "Epoch [74/1000], Train Loss: 0.0425, Val Loss: 0.0472\n",
      "Best model saved with Val Loss: 0.0472\n",
      "Epoch [75/1000], Train Loss: 0.0428, Val Loss: 0.0471\n",
      "Best model saved with Val Loss: 0.0471\n",
      "Epoch [76/1000], Train Loss: 0.0415, Val Loss: 0.0471\n",
      "Epoch [77/1000], Train Loss: 0.0424, Val Loss: 0.0472\n",
      "Epoch [78/1000], Train Loss: 0.0417, Val Loss: 0.0470\n",
      "Best model saved with Val Loss: 0.0470\n",
      "Epoch [79/1000], Train Loss: 0.0417, Val Loss: 0.0472\n",
      "Epoch [80/1000], Train Loss: 0.0415, Val Loss: 0.0470\n",
      "Best model saved with Val Loss: 0.0470\n",
      "Epoch [81/1000], Train Loss: 0.0414, Val Loss: 0.0469\n",
      "Best model saved with Val Loss: 0.0469\n",
      "Epoch [82/1000], Train Loss: 0.0415, Val Loss: 0.0469\n",
      "Best model saved with Val Loss: 0.0469\n",
      "Epoch [83/1000], Train Loss: 0.0415, Val Loss: 0.0469\n",
      "Epoch [84/1000], Train Loss: 0.0412, Val Loss: 0.0469\n",
      "Epoch [85/1000], Train Loss: 0.0553, Val Loss: 0.0469\n",
      "Epoch [86/1000], Train Loss: 0.0436, Val Loss: 0.0470\n",
      "Epoch [87/1000], Train Loss: 0.0422, Val Loss: 0.0471\n",
      "Epoch [88/1000], Train Loss: 0.0452, Val Loss: 0.0469\n",
      "Epoch [89/1000], Train Loss: 0.0426, Val Loss: 0.0469\n",
      "Epoch [90/1000], Train Loss: 0.0441, Val Loss: 0.0468\n",
      "Best model saved with Val Loss: 0.0468\n",
      "Epoch [91/1000], Train Loss: 0.0425, Val Loss: 0.0467\n",
      "Best model saved with Val Loss: 0.0467\n",
      "Epoch [92/1000], Train Loss: 0.0423, Val Loss: 0.0467\n",
      "Epoch [93/1000], Train Loss: 0.0410, Val Loss: 0.0469\n",
      "Epoch [94/1000], Train Loss: 0.0413, Val Loss: 0.0465\n",
      "Best model saved with Val Loss: 0.0465\n",
      "Epoch [95/1000], Train Loss: 0.0418, Val Loss: 0.0467\n",
      "Epoch [96/1000], Train Loss: 0.0446, Val Loss: 0.0466\n",
      "Epoch [97/1000], Train Loss: 0.0433, Val Loss: 0.0466\n",
      "Epoch [98/1000], Train Loss: 0.0416, Val Loss: 0.0465\n",
      "Best model saved with Val Loss: 0.0465\n",
      "Epoch [99/1000], Train Loss: 0.0408, Val Loss: 0.0464\n",
      "Best model saved with Val Loss: 0.0464\n",
      "Epoch [100/1000], Train Loss: 0.0417, Val Loss: 0.0466\n",
      "Epoch [101/1000], Train Loss: 0.0409, Val Loss: 0.0466\n",
      "Epoch [102/1000], Train Loss: 0.0418, Val Loss: 0.0466\n",
      "Epoch [103/1000], Train Loss: 0.0410, Val Loss: 0.0465\n",
      "Epoch [104/1000], Train Loss: 0.0410, Val Loss: 0.0463\n",
      "Best model saved with Val Loss: 0.0463\n",
      "Epoch [105/1000], Train Loss: 0.0409, Val Loss: 0.0466\n",
      "Epoch [106/1000], Train Loss: 0.0438, Val Loss: 0.0463\n",
      "Epoch [107/1000], Train Loss: 0.0425, Val Loss: 0.0464\n",
      "Epoch [108/1000], Train Loss: 0.0409, Val Loss: 0.0463\n",
      "Epoch [109/1000], Train Loss: 0.0410, Val Loss: 0.0462\n",
      "Best model saved with Val Loss: 0.0462\n",
      "Epoch [110/1000], Train Loss: 0.0410, Val Loss: 0.0463\n",
      "Epoch [111/1000], Train Loss: 0.0417, Val Loss: 0.0464\n",
      "Epoch [112/1000], Train Loss: 0.0421, Val Loss: 0.0464\n",
      "Epoch [113/1000], Train Loss: 0.0411, Val Loss: 0.0461\n",
      "Best model saved with Val Loss: 0.0461\n",
      "Epoch [114/1000], Train Loss: 0.0417, Val Loss: 0.0463\n",
      "Epoch [115/1000], Train Loss: 0.0418, Val Loss: 0.0462\n",
      "Epoch [116/1000], Train Loss: 0.0426, Val Loss: 0.0462\n",
      "Epoch [117/1000], Train Loss: 0.0412, Val Loss: 0.0461\n",
      "Epoch [118/1000], Train Loss: 0.0411, Val Loss: 0.0463\n",
      "Epoch [119/1000], Train Loss: 0.0410, Val Loss: 0.0463\n",
      "Epoch [120/1000], Train Loss: 0.0407, Val Loss: 0.0463\n",
      "Epoch [121/1000], Train Loss: 0.0408, Val Loss: 0.0462\n",
      "Epoch [122/1000], Train Loss: 0.0415, Val Loss: 0.0461\n",
      "Best model saved with Val Loss: 0.0461\n",
      "Epoch [123/1000], Train Loss: 0.0406, Val Loss: 0.0462\n",
      "Epoch [124/1000], Train Loss: 0.0405, Val Loss: 0.0460\n",
      "Best model saved with Val Loss: 0.0460\n",
      "Epoch [125/1000], Train Loss: 0.0408, Val Loss: 0.0460\n",
      "Best model saved with Val Loss: 0.0460\n",
      "Epoch [126/1000], Train Loss: 0.0441, Val Loss: 0.0460\n",
      "Epoch [127/1000], Train Loss: 0.0409, Val Loss: 0.0462\n",
      "Epoch [128/1000], Train Loss: 0.0408, Val Loss: 0.0462\n",
      "Epoch [129/1000], Train Loss: 0.0409, Val Loss: 0.0461\n",
      "Epoch [130/1000], Train Loss: 0.0407, Val Loss: 0.0463\n",
      "Epoch [131/1000], Train Loss: 0.0405, Val Loss: 0.0461\n",
      "Epoch [132/1000], Train Loss: 0.0408, Val Loss: 0.0461\n",
      "Epoch [133/1000], Train Loss: 0.0408, Val Loss: 0.0459\n",
      "Best model saved with Val Loss: 0.0459\n",
      "Epoch [134/1000], Train Loss: 0.0447, Val Loss: 0.0459\n",
      "Epoch [135/1000], Train Loss: 0.0404, Val Loss: 0.0461\n",
      "Epoch [136/1000], Train Loss: 0.0420, Val Loss: 0.0459\n",
      "Best model saved with Val Loss: 0.0459\n",
      "Epoch [137/1000], Train Loss: 0.0413, Val Loss: 0.0459\n",
      "Epoch [138/1000], Train Loss: 0.0404, Val Loss: 0.0460\n",
      "Epoch [139/1000], Train Loss: 0.0417, Val Loss: 0.0460\n",
      "Epoch [140/1000], Train Loss: 0.0429, Val Loss: 0.0460\n",
      "Epoch [141/1000], Train Loss: 0.0418, Val Loss: 0.0458\n",
      "Best model saved with Val Loss: 0.0458\n",
      "Epoch [142/1000], Train Loss: 0.0421, Val Loss: 0.0459\n",
      "Epoch [143/1000], Train Loss: 0.0414, Val Loss: 0.0458\n",
      "Best model saved with Val Loss: 0.0458\n",
      "Epoch [144/1000], Train Loss: 0.0423, Val Loss: 0.0459\n",
      "Epoch [145/1000], Train Loss: 0.0431, Val Loss: 0.0462\n",
      "Epoch [146/1000], Train Loss: 0.0416, Val Loss: 0.0458\n",
      "Epoch [147/1000], Train Loss: 0.0418, Val Loss: 0.0458\n",
      "Best model saved with Val Loss: 0.0458\n",
      "Epoch [148/1000], Train Loss: 0.0407, Val Loss: 0.0458\n",
      "Best model saved with Val Loss: 0.0458\n",
      "Epoch [149/1000], Train Loss: 0.0412, Val Loss: 0.0459\n",
      "Epoch [150/1000], Train Loss: 0.0409, Val Loss: 0.0457\n",
      "Best model saved with Val Loss: 0.0457\n",
      "Epoch [151/1000], Train Loss: 0.0407, Val Loss: 0.0457\n",
      "Epoch [152/1000], Train Loss: 0.0431, Val Loss: 0.0457\n",
      "Epoch [153/1000], Train Loss: 0.0406, Val Loss: 0.0457\n",
      "Best model saved with Val Loss: 0.0457\n",
      "Epoch [154/1000], Train Loss: 0.0410, Val Loss: 0.0457\n",
      "Epoch [155/1000], Train Loss: 0.0418, Val Loss: 0.0462\n",
      "Epoch [156/1000], Train Loss: 0.0406, Val Loss: 0.0458\n",
      "Epoch [157/1000], Train Loss: 0.0411, Val Loss: 0.0457\n",
      "Epoch [158/1000], Train Loss: 0.0410, Val Loss: 0.0457\n",
      "Best model saved with Val Loss: 0.0457\n",
      "Epoch [159/1000], Train Loss: 0.0408, Val Loss: 0.0455\n",
      "Best model saved with Val Loss: 0.0455\n",
      "Epoch [160/1000], Train Loss: 0.0404, Val Loss: 0.0462\n",
      "Epoch [161/1000], Train Loss: 0.0411, Val Loss: 0.0457\n",
      "Epoch [162/1000], Train Loss: 0.0402, Val Loss: 0.0459\n",
      "Epoch [163/1000], Train Loss: 0.0401, Val Loss: 0.0457\n",
      "Epoch [164/1000], Train Loss: 0.0403, Val Loss: 0.0457\n",
      "Epoch [165/1000], Train Loss: 0.0409, Val Loss: 0.0457\n",
      "Epoch [166/1000], Train Loss: 0.0410, Val Loss: 0.0457\n",
      "Epoch [167/1000], Train Loss: 0.0414, Val Loss: 0.0454\n",
      "Best model saved with Val Loss: 0.0454\n",
      "Epoch [168/1000], Train Loss: 0.0415, Val Loss: 0.0458\n",
      "Epoch [169/1000], Train Loss: 0.0407, Val Loss: 0.0457\n",
      "Epoch [170/1000], Train Loss: 0.0447, Val Loss: 0.0460\n",
      "Epoch [171/1000], Train Loss: 0.0414, Val Loss: 0.0458\n",
      "Epoch [172/1000], Train Loss: 0.0416, Val Loss: 0.0457\n",
      "Epoch [173/1000], Train Loss: 0.0402, Val Loss: 0.0459\n",
      "Epoch [174/1000], Train Loss: 0.0407, Val Loss: 0.0456\n",
      "Epoch [175/1000], Train Loss: 0.0410, Val Loss: 0.0456\n",
      "Epoch [176/1000], Train Loss: 0.0418, Val Loss: 0.0453\n",
      "Best model saved with Val Loss: 0.0453\n",
      "Epoch [177/1000], Train Loss: 0.0428, Val Loss: 0.0455\n",
      "Epoch [178/1000], Train Loss: 0.0401, Val Loss: 0.0456\n",
      "Epoch [179/1000], Train Loss: 0.0405, Val Loss: 0.0455\n",
      "Epoch [180/1000], Train Loss: 0.0412, Val Loss: 0.0460\n",
      "Epoch [181/1000], Train Loss: 0.0404, Val Loss: 0.0455\n",
      "Epoch [182/1000], Train Loss: 0.0411, Val Loss: 0.0454\n",
      "Epoch [183/1000], Train Loss: 0.0420, Val Loss: 0.0455\n",
      "Epoch [184/1000], Train Loss: 0.0430, Val Loss: 0.0457\n",
      "Epoch [185/1000], Train Loss: 0.0399, Val Loss: 0.0454\n",
      "Epoch [186/1000], Train Loss: 0.0418, Val Loss: 0.0459\n",
      "Epoch [187/1000], Train Loss: 0.0403, Val Loss: 0.0455\n",
      "Epoch [188/1000], Train Loss: 0.0427, Val Loss: 0.0458\n",
      "Epoch [189/1000], Train Loss: 0.0422, Val Loss: 0.0453\n",
      "Epoch [190/1000], Train Loss: 0.0403, Val Loss: 0.0454\n",
      "Epoch [191/1000], Train Loss: 0.0441, Val Loss: 0.0454\n",
      "Epoch [192/1000], Train Loss: 0.0403, Val Loss: 0.0462\n",
      "Epoch [193/1000], Train Loss: 0.0406, Val Loss: 0.0458\n",
      "Epoch [194/1000], Train Loss: 0.0408, Val Loss: 0.0456\n",
      "Epoch [195/1000], Train Loss: 0.0414, Val Loss: 0.0457\n",
      "Epoch [196/1000], Train Loss: 0.0404, Val Loss: 0.0452\n",
      "Best model saved with Val Loss: 0.0452\n",
      "Epoch [197/1000], Train Loss: 0.0444, Val Loss: 0.0457\n",
      "Epoch [198/1000], Train Loss: 0.0411, Val Loss: 0.0457\n",
      "Epoch [199/1000], Train Loss: 0.0417, Val Loss: 0.0456\n",
      "Epoch [200/1000], Train Loss: 0.0404, Val Loss: 0.0453\n",
      "Epoch [201/1000], Train Loss: 0.0441, Val Loss: 0.0454\n",
      "Epoch [202/1000], Train Loss: 0.0405, Val Loss: 0.0455\n",
      "Epoch [203/1000], Train Loss: 0.0414, Val Loss: 0.0459\n",
      "Epoch [204/1000], Train Loss: 0.0414, Val Loss: 0.0454\n",
      "Epoch [205/1000], Train Loss: 0.0402, Val Loss: 0.0454\n",
      "Epoch [206/1000], Train Loss: 0.0427, Val Loss: 0.0454\n",
      "Epoch [207/1000], Train Loss: 0.0405, Val Loss: 0.0456\n",
      "Epoch [208/1000], Train Loss: 0.0423, Val Loss: 0.0452\n",
      "Best model saved with Val Loss: 0.0452\n",
      "Epoch [209/1000], Train Loss: 0.0416, Val Loss: 0.0456\n",
      "Epoch [210/1000], Train Loss: 0.0406, Val Loss: 0.0453\n",
      "Epoch [211/1000], Train Loss: 0.0403, Val Loss: 0.0456\n",
      "Epoch [212/1000], Train Loss: 0.0402, Val Loss: 0.0453\n",
      "Epoch [213/1000], Train Loss: 0.0403, Val Loss: 0.0458\n",
      "Epoch [214/1000], Train Loss: 0.0402, Val Loss: 0.0453\n",
      "Epoch [215/1000], Train Loss: 0.0409, Val Loss: 0.0458\n",
      "Epoch [216/1000], Train Loss: 0.0398, Val Loss: 0.0453\n",
      "Epoch [217/1000], Train Loss: 0.0400, Val Loss: 0.0455\n",
      "Epoch [218/1000], Train Loss: 0.0403, Val Loss: 0.0451\n",
      "Best model saved with Val Loss: 0.0451\n",
      "Epoch [219/1000], Train Loss: 0.0399, Val Loss: 0.0456\n",
      "Epoch [220/1000], Train Loss: 0.0406, Val Loss: 0.0453\n",
      "Epoch [221/1000], Train Loss: 0.0415, Val Loss: 0.0453\n",
      "Epoch [222/1000], Train Loss: 0.0398, Val Loss: 0.0456\n",
      "Epoch [223/1000], Train Loss: 0.0399, Val Loss: 0.0454\n",
      "Epoch [224/1000], Train Loss: 0.0400, Val Loss: 0.0452\n",
      "Epoch [225/1000], Train Loss: 0.0399, Val Loss: 0.0455\n",
      "Epoch [226/1000], Train Loss: 0.0398, Val Loss: 0.0454\n",
      "Epoch [227/1000], Train Loss: 0.0412, Val Loss: 0.0454\n",
      "Epoch [228/1000], Train Loss: 0.0446, Val Loss: 0.0452\n",
      "Epoch [229/1000], Train Loss: 0.0405, Val Loss: 0.0453\n",
      "Epoch [230/1000], Train Loss: 0.0407, Val Loss: 0.0458\n",
      "Epoch [231/1000], Train Loss: 0.0400, Val Loss: 0.0456\n",
      "Epoch [232/1000], Train Loss: 0.0399, Val Loss: 0.0454\n",
      "Epoch [233/1000], Train Loss: 0.0399, Val Loss: 0.0457\n",
      "Epoch [234/1000], Train Loss: 0.0400, Val Loss: 0.0454\n",
      "Epoch [235/1000], Train Loss: 0.0397, Val Loss: 0.0452\n",
      "Epoch [236/1000], Train Loss: 0.0421, Val Loss: 0.0457\n",
      "Epoch [237/1000], Train Loss: 0.0414, Val Loss: 0.0450\n",
      "Best model saved with Val Loss: 0.0450\n",
      "Epoch [238/1000], Train Loss: 0.0406, Val Loss: 0.0457\n",
      "Epoch [239/1000], Train Loss: 0.0405, Val Loss: 0.0452\n",
      "Epoch [240/1000], Train Loss: 0.0406, Val Loss: 0.0458\n",
      "Epoch [241/1000], Train Loss: 0.0407, Val Loss: 0.0451\n",
      "Epoch [242/1000], Train Loss: 0.0398, Val Loss: 0.0455\n",
      "Epoch [243/1000], Train Loss: 0.0419, Val Loss: 0.0454\n",
      "Epoch [244/1000], Train Loss: 0.0399, Val Loss: 0.0453\n",
      "Epoch [245/1000], Train Loss: 0.0400, Val Loss: 0.0453\n",
      "Epoch [246/1000], Train Loss: 0.0405, Val Loss: 0.0452\n",
      "Epoch [247/1000], Train Loss: 0.0425, Val Loss: 0.0454\n",
      "Epoch [248/1000], Train Loss: 0.0400, Val Loss: 0.0451\n",
      "Epoch [249/1000], Train Loss: 0.0410, Val Loss: 0.0454\n",
      "Epoch [250/1000], Train Loss: 0.0413, Val Loss: 0.0451\n",
      "Epoch [251/1000], Train Loss: 0.0399, Val Loss: 0.0454\n",
      "Epoch [252/1000], Train Loss: 0.0404, Val Loss: 0.0451\n",
      "Epoch [253/1000], Train Loss: 0.0402, Val Loss: 0.0455\n",
      "Epoch [254/1000], Train Loss: 0.0402, Val Loss: 0.0451\n",
      "Epoch [255/1000], Train Loss: 0.0402, Val Loss: 0.0451\n",
      "Epoch [256/1000], Train Loss: 0.0434, Val Loss: 0.0460\n",
      "Epoch [257/1000], Train Loss: 0.0415, Val Loss: 0.0451\n",
      "Epoch [258/1000], Train Loss: 0.0402, Val Loss: 0.0454\n",
      "Epoch [259/1000], Train Loss: 0.0410, Val Loss: 0.0450\n",
      "Epoch [260/1000], Train Loss: 0.0526, Val Loss: 0.0452\n",
      "Epoch [261/1000], Train Loss: 0.0428, Val Loss: 0.0452\n",
      "Epoch [262/1000], Train Loss: 0.0399, Val Loss: 0.0454\n",
      "Epoch [263/1000], Train Loss: 0.0401, Val Loss: 0.0452\n",
      "Epoch [264/1000], Train Loss: 0.0445, Val Loss: 0.0451\n",
      "Epoch [265/1000], Train Loss: 0.0398, Val Loss: 0.0452\n",
      "Epoch [266/1000], Train Loss: 0.0414, Val Loss: 0.0456\n",
      "Epoch [267/1000], Train Loss: 0.0418, Val Loss: 0.0449\n",
      "Best model saved with Val Loss: 0.0449\n",
      "Epoch [268/1000], Train Loss: 0.0414, Val Loss: 0.0455\n",
      "Epoch [269/1000], Train Loss: 0.0396, Val Loss: 0.0454\n",
      "Epoch [270/1000], Train Loss: 0.0401, Val Loss: 0.0455\n",
      "Epoch [271/1000], Train Loss: 0.0397, Val Loss: 0.0452\n",
      "Epoch [272/1000], Train Loss: 0.0396, Val Loss: 0.0452\n",
      "Epoch [273/1000], Train Loss: 0.0407, Val Loss: 0.0455\n",
      "Epoch [274/1000], Train Loss: 0.0415, Val Loss: 0.0451\n",
      "Epoch [275/1000], Train Loss: 0.0405, Val Loss: 0.0454\n",
      "Epoch [276/1000], Train Loss: 0.0414, Val Loss: 0.0450\n",
      "Epoch [277/1000], Train Loss: 0.0401, Val Loss: 0.0453\n",
      "Epoch [278/1000], Train Loss: 0.0410, Val Loss: 0.0453\n",
      "Epoch [279/1000], Train Loss: 0.0399, Val Loss: 0.0450\n",
      "Epoch [280/1000], Train Loss: 0.0403, Val Loss: 0.0453\n",
      "Epoch [281/1000], Train Loss: 0.0411, Val Loss: 0.0451\n",
      "Epoch [282/1000], Train Loss: 0.0397, Val Loss: 0.0451\n",
      "Epoch [283/1000], Train Loss: 0.0483, Val Loss: 0.0450\n",
      "Epoch [284/1000], Train Loss: 0.0399, Val Loss: 0.0454\n",
      "Epoch [285/1000], Train Loss: 0.0402, Val Loss: 0.0453\n",
      "Epoch [286/1000], Train Loss: 0.0397, Val Loss: 0.0451\n",
      "Epoch [287/1000], Train Loss: 0.0412, Val Loss: 0.0453\n",
      "Epoch [288/1000], Train Loss: 0.0405, Val Loss: 0.0450\n",
      "Epoch [289/1000], Train Loss: 0.0416, Val Loss: 0.0455\n",
      "Epoch [290/1000], Train Loss: 0.0424, Val Loss: 0.0450\n",
      "Epoch [291/1000], Train Loss: 0.0397, Val Loss: 0.0452\n",
      "Epoch [292/1000], Train Loss: 0.0407, Val Loss: 0.0451\n",
      "Epoch [293/1000], Train Loss: 0.0397, Val Loss: 0.0450\n",
      "Epoch [294/1000], Train Loss: 0.0407, Val Loss: 0.0450\n",
      "Epoch [295/1000], Train Loss: 0.0410, Val Loss: 0.0453\n",
      "Epoch [296/1000], Train Loss: 0.0417, Val Loss: 0.0451\n",
      "Epoch [297/1000], Train Loss: 0.0408, Val Loss: 0.0451\n",
      "Epoch [298/1000], Train Loss: 0.0401, Val Loss: 0.0451\n",
      "Epoch [299/1000], Train Loss: 0.0404, Val Loss: 0.0450\n",
      "Epoch [300/1000], Train Loss: 0.0405, Val Loss: 0.0448\n",
      "Best model saved with Val Loss: 0.0448\n",
      "Epoch [301/1000], Train Loss: 0.0399, Val Loss: 0.0453\n",
      "Epoch [302/1000], Train Loss: 0.0401, Val Loss: 0.0450\n",
      "Epoch [303/1000], Train Loss: 0.0395, Val Loss: 0.0451\n",
      "Epoch [304/1000], Train Loss: 0.0398, Val Loss: 0.0452\n",
      "Epoch [305/1000], Train Loss: 0.0404, Val Loss: 0.0450\n",
      "Epoch [306/1000], Train Loss: 0.0405, Val Loss: 0.0455\n",
      "Epoch [307/1000], Train Loss: 0.0403, Val Loss: 0.0448\n",
      "Best model saved with Val Loss: 0.0448\n",
      "Epoch [308/1000], Train Loss: 0.0413, Val Loss: 0.0451\n",
      "Epoch [309/1000], Train Loss: 0.0405, Val Loss: 0.0451\n",
      "Epoch [310/1000], Train Loss: 0.0400, Val Loss: 0.0452\n",
      "Epoch [311/1000], Train Loss: 0.0400, Val Loss: 0.0449\n",
      "Epoch [312/1000], Train Loss: 0.0395, Val Loss: 0.0449\n",
      "Epoch [313/1000], Train Loss: 0.0401, Val Loss: 0.0452\n",
      "Epoch [314/1000], Train Loss: 0.0402, Val Loss: 0.0449\n",
      "Epoch [315/1000], Train Loss: 0.0395, Val Loss: 0.0451\n",
      "Epoch [316/1000], Train Loss: 0.0420, Val Loss: 0.0450\n",
      "Epoch [317/1000], Train Loss: 0.0404, Val Loss: 0.0454\n",
      "Epoch [318/1000], Train Loss: 0.0401, Val Loss: 0.0451\n",
      "Epoch [319/1000], Train Loss: 0.0412, Val Loss: 0.0449\n",
      "Epoch [320/1000], Train Loss: 0.0399, Val Loss: 0.0459\n",
      "Epoch [321/1000], Train Loss: 0.0395, Val Loss: 0.0449\n",
      "Epoch [322/1000], Train Loss: 0.0420, Val Loss: 0.0448\n",
      "Epoch [323/1000], Train Loss: 0.0485, Val Loss: 0.0450\n",
      "Epoch [324/1000], Train Loss: 0.0395, Val Loss: 0.0448\n",
      "Epoch [325/1000], Train Loss: 0.0402, Val Loss: 0.0453\n",
      "Epoch [326/1000], Train Loss: 0.0421, Val Loss: 0.0449\n",
      "Epoch [327/1000], Train Loss: 0.0396, Val Loss: 0.0452\n",
      "Epoch [328/1000], Train Loss: 0.0434, Val Loss: 0.0449\n",
      "Epoch [329/1000], Train Loss: 0.0398, Val Loss: 0.0452\n",
      "Epoch [330/1000], Train Loss: 0.0393, Val Loss: 0.0450\n",
      "Epoch [331/1000], Train Loss: 0.0408, Val Loss: 0.0451\n",
      "Epoch [332/1000], Train Loss: 0.0402, Val Loss: 0.0448\n",
      "Best model saved with Val Loss: 0.0448\n",
      "Epoch [333/1000], Train Loss: 0.0402, Val Loss: 0.0451\n",
      "Epoch [334/1000], Train Loss: 0.0404, Val Loss: 0.0450\n",
      "Epoch [335/1000], Train Loss: 0.0396, Val Loss: 0.0452\n",
      "Epoch [336/1000], Train Loss: 0.0404, Val Loss: 0.0450\n",
      "Epoch [337/1000], Train Loss: 0.0402, Val Loss: 0.0448\n",
      "Epoch [338/1000], Train Loss: 0.0395, Val Loss: 0.0449\n",
      "Epoch [339/1000], Train Loss: 0.0398, Val Loss: 0.0450\n",
      "Epoch [340/1000], Train Loss: 0.0399, Val Loss: 0.0450\n",
      "Epoch [341/1000], Train Loss: 0.0433, Val Loss: 0.0454\n",
      "Epoch [342/1000], Train Loss: 0.0406, Val Loss: 0.0451\n",
      "Epoch [343/1000], Train Loss: 0.0404, Val Loss: 0.0449\n",
      "Epoch [344/1000], Train Loss: 0.0406, Val Loss: 0.0453\n",
      "Epoch [345/1000], Train Loss: 0.0433, Val Loss: 0.0448\n",
      "Best model saved with Val Loss: 0.0448\n",
      "Epoch [346/1000], Train Loss: 0.0410, Val Loss: 0.0449\n",
      "Epoch [347/1000], Train Loss: 0.0414, Val Loss: 0.0449\n",
      "Epoch [348/1000], Train Loss: 0.0414, Val Loss: 0.0452\n",
      "Epoch [349/1000], Train Loss: 0.0413, Val Loss: 0.0447\n",
      "Best model saved with Val Loss: 0.0447\n",
      "Epoch [350/1000], Train Loss: 0.0405, Val Loss: 0.0448\n",
      "Epoch [351/1000], Train Loss: 0.0404, Val Loss: 0.0449\n",
      "Epoch [352/1000], Train Loss: 0.0416, Val Loss: 0.0449\n",
      "Epoch [353/1000], Train Loss: 0.0397, Val Loss: 0.0448\n",
      "Epoch [354/1000], Train Loss: 0.0404, Val Loss: 0.0448\n",
      "Epoch [355/1000], Train Loss: 0.0395, Val Loss: 0.0450\n",
      "Epoch [356/1000], Train Loss: 0.0394, Val Loss: 0.0447\n",
      "Epoch [357/1000], Train Loss: 0.0407, Val Loss: 0.0448\n",
      "Epoch [358/1000], Train Loss: 0.0402, Val Loss: 0.0448\n",
      "Epoch [359/1000], Train Loss: 0.0413, Val Loss: 0.0450\n",
      "Epoch [360/1000], Train Loss: 0.0396, Val Loss: 0.0451\n",
      "Epoch [361/1000], Train Loss: 0.0409, Val Loss: 0.0449\n",
      "Epoch [362/1000], Train Loss: 0.0396, Val Loss: 0.0449\n",
      "Epoch [363/1000], Train Loss: 0.0401, Val Loss: 0.0449\n",
      "Epoch [364/1000], Train Loss: 0.0409, Val Loss: 0.0446\n",
      "Best model saved with Val Loss: 0.0446\n",
      "Epoch [365/1000], Train Loss: 0.0394, Val Loss: 0.0447\n",
      "Epoch [366/1000], Train Loss: 0.0395, Val Loss: 0.0452\n",
      "Epoch [367/1000], Train Loss: 0.0412, Val Loss: 0.0447\n",
      "Epoch [368/1000], Train Loss: 0.0399, Val Loss: 0.0449\n",
      "Epoch [369/1000], Train Loss: 0.0397, Val Loss: 0.0447\n",
      "Epoch [370/1000], Train Loss: 0.0394, Val Loss: 0.0447\n",
      "Epoch [371/1000], Train Loss: 0.0393, Val Loss: 0.0448\n",
      "Epoch [372/1000], Train Loss: 0.0394, Val Loss: 0.0449\n",
      "Epoch [373/1000], Train Loss: 0.0405, Val Loss: 0.0448\n",
      "Epoch [374/1000], Train Loss: 0.0402, Val Loss: 0.0447\n",
      "Epoch [375/1000], Train Loss: 0.0397, Val Loss: 0.0446\n",
      "Best model saved with Val Loss: 0.0446\n",
      "Epoch [376/1000], Train Loss: 0.0404, Val Loss: 0.0451\n",
      "Epoch [377/1000], Train Loss: 0.0401, Val Loss: 0.0446\n",
      "Epoch [378/1000], Train Loss: 0.0395, Val Loss: 0.0450\n",
      "Epoch [379/1000], Train Loss: 0.0416, Val Loss: 0.0450\n",
      "Epoch [380/1000], Train Loss: 0.0405, Val Loss: 0.0447\n",
      "Epoch [381/1000], Train Loss: 0.0394, Val Loss: 0.0452\n",
      "Epoch [382/1000], Train Loss: 0.0399, Val Loss: 0.0445\n",
      "Best model saved with Val Loss: 0.0445\n",
      "Epoch [383/1000], Train Loss: 0.0392, Val Loss: 0.0449\n",
      "Epoch [384/1000], Train Loss: 0.0392, Val Loss: 0.0449\n",
      "Epoch [385/1000], Train Loss: 0.0398, Val Loss: 0.0447\n",
      "Epoch [386/1000], Train Loss: 0.0393, Val Loss: 0.0448\n",
      "Epoch [387/1000], Train Loss: 0.0392, Val Loss: 0.0446\n",
      "Epoch [388/1000], Train Loss: 0.0410, Val Loss: 0.0447\n",
      "Epoch [389/1000], Train Loss: 0.0430, Val Loss: 0.0445\n",
      "Best model saved with Val Loss: 0.0445\n",
      "Epoch [390/1000], Train Loss: 0.0408, Val Loss: 0.0445\n",
      "Epoch [391/1000], Train Loss: 0.0409, Val Loss: 0.0449\n",
      "Epoch [392/1000], Train Loss: 0.0403, Val Loss: 0.0446\n",
      "Epoch [393/1000], Train Loss: 0.0411, Val Loss: 0.0448\n",
      "Epoch [394/1000], Train Loss: 0.0394, Val Loss: 0.0446\n",
      "Epoch [395/1000], Train Loss: 0.0424, Val Loss: 0.0448\n",
      "Epoch [396/1000], Train Loss: 0.0399, Val Loss: 0.0447\n",
      "Epoch [397/1000], Train Loss: 0.0397, Val Loss: 0.0448\n",
      "Epoch [398/1000], Train Loss: 0.0395, Val Loss: 0.0444\n",
      "Best model saved with Val Loss: 0.0444\n",
      "Epoch [399/1000], Train Loss: 0.0393, Val Loss: 0.0450\n",
      "Epoch [400/1000], Train Loss: 0.0391, Val Loss: 0.0448\n",
      "Epoch [401/1000], Train Loss: 0.0414, Val Loss: 0.0448\n",
      "Epoch [402/1000], Train Loss: 0.0405, Val Loss: 0.0450\n",
      "Epoch [403/1000], Train Loss: 0.0438, Val Loss: 0.0447\n",
      "Epoch [404/1000], Train Loss: 0.0415, Val Loss: 0.0446\n",
      "Epoch [405/1000], Train Loss: 0.0412, Val Loss: 0.0449\n",
      "Epoch [406/1000], Train Loss: 0.0409, Val Loss: 0.0446\n",
      "Epoch [407/1000], Train Loss: 0.0407, Val Loss: 0.0449\n",
      "Epoch [408/1000], Train Loss: 0.0407, Val Loss: 0.0453\n",
      "Epoch [409/1000], Train Loss: 0.0410, Val Loss: 0.0445\n",
      "Epoch [410/1000], Train Loss: 0.0402, Val Loss: 0.0449\n",
      "Epoch [411/1000], Train Loss: 0.0398, Val Loss: 0.0446\n",
      "Epoch [412/1000], Train Loss: 0.0398, Val Loss: 0.0448\n",
      "Epoch [413/1000], Train Loss: 0.0395, Val Loss: 0.0447\n",
      "Epoch [414/1000], Train Loss: 0.0403, Val Loss: 0.0447\n",
      "Epoch [415/1000], Train Loss: 0.0398, Val Loss: 0.0448\n",
      "Epoch [416/1000], Train Loss: 0.0393, Val Loss: 0.0449\n",
      "Epoch [417/1000], Train Loss: 0.0392, Val Loss: 0.0448\n",
      "Epoch [418/1000], Train Loss: 0.0404, Val Loss: 0.0449\n",
      "Epoch [419/1000], Train Loss: 0.0397, Val Loss: 0.0446\n",
      "Epoch [420/1000], Train Loss: 0.0398, Val Loss: 0.0446\n",
      "Epoch [421/1000], Train Loss: 0.0402, Val Loss: 0.0449\n",
      "Epoch [422/1000], Train Loss: 0.0406, Val Loss: 0.0447\n",
      "Epoch [423/1000], Train Loss: 0.0392, Val Loss: 0.0449\n",
      "Epoch [424/1000], Train Loss: 0.0395, Val Loss: 0.0446\n",
      "Epoch [425/1000], Train Loss: 0.0394, Val Loss: 0.0444\n",
      "Best model saved with Val Loss: 0.0444\n",
      "Epoch [426/1000], Train Loss: 0.0397, Val Loss: 0.0447\n",
      "Epoch [427/1000], Train Loss: 0.0403, Val Loss: 0.0446\n",
      "Epoch [428/1000], Train Loss: 0.0394, Val Loss: 0.0445\n",
      "Epoch [429/1000], Train Loss: 0.0394, Val Loss: 0.0443\n",
      "Best model saved with Val Loss: 0.0443\n",
      "Epoch [430/1000], Train Loss: 0.0399, Val Loss: 0.0446\n",
      "Epoch [431/1000], Train Loss: 0.0407, Val Loss: 0.0445\n",
      "Epoch [432/1000], Train Loss: 0.0409, Val Loss: 0.0445\n",
      "Epoch [433/1000], Train Loss: 0.0407, Val Loss: 0.0445\n",
      "Epoch [434/1000], Train Loss: 0.0429, Val Loss: 0.0446\n",
      "Epoch [435/1000], Train Loss: 0.0399, Val Loss: 0.0449\n",
      "Epoch [436/1000], Train Loss: 0.0413, Val Loss: 0.0447\n",
      "Epoch [437/1000], Train Loss: 0.0395, Val Loss: 0.0447\n",
      "Epoch [438/1000], Train Loss: 0.0403, Val Loss: 0.0450\n",
      "Epoch [439/1000], Train Loss: 0.0400, Val Loss: 0.0445\n",
      "Epoch [440/1000], Train Loss: 0.0406, Val Loss: 0.0447\n",
      "Epoch [441/1000], Train Loss: 0.0403, Val Loss: 0.0446\n",
      "Epoch [442/1000], Train Loss: 0.0395, Val Loss: 0.0449\n",
      "Epoch [443/1000], Train Loss: 0.0400, Val Loss: 0.0446\n",
      "Epoch [444/1000], Train Loss: 0.0425, Val Loss: 0.0451\n",
      "Epoch [445/1000], Train Loss: 0.0402, Val Loss: 0.0450\n",
      "Epoch [446/1000], Train Loss: 0.0407, Val Loss: 0.0447\n",
      "Epoch [447/1000], Train Loss: 0.0399, Val Loss: 0.0446\n",
      "Epoch [448/1000], Train Loss: 0.0391, Val Loss: 0.0443\n",
      "Best model saved with Val Loss: 0.0443\n",
      "Epoch [449/1000], Train Loss: 0.0393, Val Loss: 0.0447\n",
      "Epoch [450/1000], Train Loss: 0.0411, Val Loss: 0.0447\n",
      "Epoch [451/1000], Train Loss: 0.0392, Val Loss: 0.0445\n",
      "Epoch [452/1000], Train Loss: 0.0401, Val Loss: 0.0445\n",
      "Epoch [453/1000], Train Loss: 0.0394, Val Loss: 0.0445\n",
      "Epoch [454/1000], Train Loss: 0.0393, Val Loss: 0.0445\n",
      "Epoch [455/1000], Train Loss: 0.0403, Val Loss: 0.0445\n",
      "Epoch [456/1000], Train Loss: 0.0394, Val Loss: 0.0447\n",
      "Epoch [457/1000], Train Loss: 0.0391, Val Loss: 0.0445\n",
      "Epoch [458/1000], Train Loss: 0.0395, Val Loss: 0.0443\n",
      "Epoch [459/1000], Train Loss: 0.0408, Val Loss: 0.0447\n",
      "Epoch [460/1000], Train Loss: 0.0396, Val Loss: 0.0446\n",
      "Epoch [461/1000], Train Loss: 0.0399, Val Loss: 0.0444\n",
      "Epoch [462/1000], Train Loss: 0.0394, Val Loss: 0.0444\n",
      "Epoch [463/1000], Train Loss: 0.0401, Val Loss: 0.0447\n",
      "Epoch [464/1000], Train Loss: 0.0394, Val Loss: 0.0444\n",
      "Epoch [465/1000], Train Loss: 0.0399, Val Loss: 0.0445\n",
      "Epoch [466/1000], Train Loss: 0.0391, Val Loss: 0.0444\n",
      "Epoch [467/1000], Train Loss: 0.0390, Val Loss: 0.0444\n",
      "Epoch [468/1000], Train Loss: 0.0506, Val Loss: 0.0444\n",
      "Epoch [469/1000], Train Loss: 0.0411, Val Loss: 0.0450\n",
      "Epoch [470/1000], Train Loss: 0.0403, Val Loss: 0.0445\n",
      "Epoch [471/1000], Train Loss: 0.0404, Val Loss: 0.0444\n",
      "Epoch [472/1000], Train Loss: 0.0409, Val Loss: 0.0446\n",
      "Epoch [473/1000], Train Loss: 0.0393, Val Loss: 0.0444\n",
      "Epoch [474/1000], Train Loss: 0.0391, Val Loss: 0.0445\n",
      "Epoch [475/1000], Train Loss: 0.0411, Val Loss: 0.0447\n",
      "Epoch [476/1000], Train Loss: 0.0394, Val Loss: 0.0445\n",
      "Epoch [477/1000], Train Loss: 0.0401, Val Loss: 0.0446\n",
      "Epoch [478/1000], Train Loss: 0.0414, Val Loss: 0.0443\n",
      "Epoch [479/1000], Train Loss: 0.0407, Val Loss: 0.0444\n",
      "Epoch [480/1000], Train Loss: 0.0418, Val Loss: 0.0445\n",
      "Epoch [481/1000], Train Loss: 0.0395, Val Loss: 0.0441\n",
      "Best model saved with Val Loss: 0.0441\n",
      "Epoch [482/1000], Train Loss: 0.0390, Val Loss: 0.0444\n",
      "Epoch [483/1000], Train Loss: 0.0402, Val Loss: 0.0444\n",
      "Epoch [484/1000], Train Loss: 0.0397, Val Loss: 0.0447\n",
      "Epoch [485/1000], Train Loss: 0.0394, Val Loss: 0.0443\n",
      "Epoch [486/1000], Train Loss: 0.0405, Val Loss: 0.0443\n",
      "Epoch [487/1000], Train Loss: 0.0389, Val Loss: 0.0444\n",
      "Epoch [488/1000], Train Loss: 0.0400, Val Loss: 0.0442\n",
      "Epoch [489/1000], Train Loss: 0.0389, Val Loss: 0.0446\n",
      "Epoch [490/1000], Train Loss: 0.0388, Val Loss: 0.0447\n",
      "Epoch [491/1000], Train Loss: 0.0399, Val Loss: 0.0442\n",
      "Epoch [492/1000], Train Loss: 0.0404, Val Loss: 0.0447\n",
      "Epoch [493/1000], Train Loss: 0.0396, Val Loss: 0.0441\n",
      "Best model saved with Val Loss: 0.0441\n",
      "Epoch [494/1000], Train Loss: 0.0412, Val Loss: 0.0444\n",
      "Epoch [495/1000], Train Loss: 0.0400, Val Loss: 0.0445\n",
      "Epoch [496/1000], Train Loss: 0.0409, Val Loss: 0.0444\n",
      "Epoch [497/1000], Train Loss: 0.0392, Val Loss: 0.0444\n",
      "Epoch [498/1000], Train Loss: 0.0406, Val Loss: 0.0441\n",
      "Best model saved with Val Loss: 0.0441\n",
      "Epoch [499/1000], Train Loss: 0.0395, Val Loss: 0.0442\n",
      "Epoch [500/1000], Train Loss: 0.0405, Val Loss: 0.0446\n",
      "Epoch [501/1000], Train Loss: 0.0392, Val Loss: 0.0448\n",
      "Epoch [502/1000], Train Loss: 0.0400, Val Loss: 0.0444\n",
      "Epoch [503/1000], Train Loss: 0.0438, Val Loss: 0.0440\n",
      "Best model saved with Val Loss: 0.0440\n",
      "Epoch [504/1000], Train Loss: 0.0398, Val Loss: 0.0450\n",
      "Epoch [505/1000], Train Loss: 0.0393, Val Loss: 0.0442\n",
      "Epoch [506/1000], Train Loss: 0.0392, Val Loss: 0.0446\n",
      "Epoch [507/1000], Train Loss: 0.0394, Val Loss: 0.0442\n",
      "Epoch [508/1000], Train Loss: 0.0392, Val Loss: 0.0442\n",
      "Epoch [509/1000], Train Loss: 0.0389, Val Loss: 0.0447\n",
      "Epoch [510/1000], Train Loss: 0.0392, Val Loss: 0.0443\n",
      "Epoch [511/1000], Train Loss: 0.0396, Val Loss: 0.0443\n",
      "Epoch [512/1000], Train Loss: 0.0388, Val Loss: 0.0442\n",
      "Epoch [513/1000], Train Loss: 0.0409, Val Loss: 0.0445\n",
      "Epoch [514/1000], Train Loss: 0.0391, Val Loss: 0.0443\n",
      "Epoch [515/1000], Train Loss: 0.0393, Val Loss: 0.0444\n",
      "Epoch [516/1000], Train Loss: 0.0389, Val Loss: 0.0445\n",
      "Epoch [517/1000], Train Loss: 0.0406, Val Loss: 0.0442\n",
      "Epoch [518/1000], Train Loss: 0.0395, Val Loss: 0.0443\n",
      "Epoch [519/1000], Train Loss: 0.0403, Val Loss: 0.0443\n",
      "Epoch [520/1000], Train Loss: 0.0403, Val Loss: 0.0444\n",
      "Epoch [521/1000], Train Loss: 0.0417, Val Loss: 0.0443\n",
      "Epoch [522/1000], Train Loss: 0.0393, Val Loss: 0.0444\n",
      "Epoch [523/1000], Train Loss: 0.0397, Val Loss: 0.0440\n",
      "Best model saved with Val Loss: 0.0440\n",
      "Epoch [524/1000], Train Loss: 0.0408, Val Loss: 0.0444\n",
      "Epoch [525/1000], Train Loss: 0.0393, Val Loss: 0.0445\n",
      "Epoch [526/1000], Train Loss: 0.0400, Val Loss: 0.0442\n",
      "Epoch [527/1000], Train Loss: 0.0391, Val Loss: 0.0445\n",
      "Epoch [528/1000], Train Loss: 0.0417, Val Loss: 0.0443\n",
      "Epoch [529/1000], Train Loss: 0.0406, Val Loss: 0.0440\n",
      "Epoch [530/1000], Train Loss: 0.0398, Val Loss: 0.0442\n",
      "Epoch [531/1000], Train Loss: 0.0410, Val Loss: 0.0442\n",
      "Epoch [532/1000], Train Loss: 0.0389, Val Loss: 0.0446\n",
      "Epoch [533/1000], Train Loss: 0.0388, Val Loss: 0.0441\n",
      "Epoch [534/1000], Train Loss: 0.0395, Val Loss: 0.0446\n",
      "Epoch [535/1000], Train Loss: 0.0435, Val Loss: 0.0442\n",
      "Epoch [536/1000], Train Loss: 0.0398, Val Loss: 0.0444\n",
      "Epoch [537/1000], Train Loss: 0.0391, Val Loss: 0.0441\n",
      "Epoch [538/1000], Train Loss: 0.0408, Val Loss: 0.0442\n",
      "Epoch [539/1000], Train Loss: 0.0388, Val Loss: 0.0442\n",
      "Epoch [540/1000], Train Loss: 0.0413, Val Loss: 0.0440\n",
      "Best model saved with Val Loss: 0.0440\n",
      "Epoch [541/1000], Train Loss: 0.0410, Val Loss: 0.0441\n",
      "Epoch [542/1000], Train Loss: 0.0388, Val Loss: 0.0442\n",
      "Epoch [543/1000], Train Loss: 0.0388, Val Loss: 0.0442\n",
      "Epoch [544/1000], Train Loss: 0.0389, Val Loss: 0.0442\n",
      "Epoch [545/1000], Train Loss: 0.0407, Val Loss: 0.0444\n",
      "Epoch [546/1000], Train Loss: 0.0393, Val Loss: 0.0441\n",
      "Epoch [547/1000], Train Loss: 0.0452, Val Loss: 0.0441\n",
      "Epoch [548/1000], Train Loss: 0.0410, Val Loss: 0.0443\n",
      "Epoch [549/1000], Train Loss: 0.0388, Val Loss: 0.0441\n",
      "Epoch [550/1000], Train Loss: 0.0391, Val Loss: 0.0441\n",
      "Epoch [551/1000], Train Loss: 0.0389, Val Loss: 0.0439\n",
      "Best model saved with Val Loss: 0.0439\n",
      "Epoch [552/1000], Train Loss: 0.0388, Val Loss: 0.0443\n",
      "Epoch [553/1000], Train Loss: 0.0393, Val Loss: 0.0441\n",
      "Epoch [554/1000], Train Loss: 0.0394, Val Loss: 0.0441\n",
      "Epoch [555/1000], Train Loss: 0.0393, Val Loss: 0.0441\n",
      "Epoch [556/1000], Train Loss: 0.0396, Val Loss: 0.0444\n",
      "Epoch [557/1000], Train Loss: 0.0396, Val Loss: 0.0441\n",
      "Epoch [558/1000], Train Loss: 0.0392, Val Loss: 0.0441\n",
      "Epoch [559/1000], Train Loss: 0.0393, Val Loss: 0.0441\n",
      "Epoch [560/1000], Train Loss: 0.0388, Val Loss: 0.0441\n",
      "Epoch [561/1000], Train Loss: 0.0412, Val Loss: 0.0443\n",
      "Epoch [562/1000], Train Loss: 0.0396, Val Loss: 0.0442\n",
      "Epoch [563/1000], Train Loss: 0.0392, Val Loss: 0.0441\n",
      "Epoch [564/1000], Train Loss: 0.0396, Val Loss: 0.0439\n",
      "Best model saved with Val Loss: 0.0439\n",
      "Epoch [565/1000], Train Loss: 0.0390, Val Loss: 0.0440\n",
      "Epoch [566/1000], Train Loss: 0.0388, Val Loss: 0.0442\n",
      "Epoch [567/1000], Train Loss: 0.0388, Val Loss: 0.0441\n",
      "Epoch [568/1000], Train Loss: 0.0410, Val Loss: 0.0441\n",
      "Epoch [569/1000], Train Loss: 0.0397, Val Loss: 0.0439\n",
      "Epoch [570/1000], Train Loss: 0.0394, Val Loss: 0.0444\n",
      "Epoch [571/1000], Train Loss: 0.0417, Val Loss: 0.0445\n",
      "Epoch [572/1000], Train Loss: 0.0391, Val Loss: 0.0441\n",
      "Epoch [573/1000], Train Loss: 0.0390, Val Loss: 0.0439\n",
      "Epoch [574/1000], Train Loss: 0.0402, Val Loss: 0.0443\n",
      "Epoch [575/1000], Train Loss: 0.0388, Val Loss: 0.0440\n",
      "Epoch [576/1000], Train Loss: 0.0514, Val Loss: 0.0440\n",
      "Epoch [577/1000], Train Loss: 0.0395, Val Loss: 0.0442\n",
      "Epoch [578/1000], Train Loss: 0.0387, Val Loss: 0.0441\n",
      "Epoch [579/1000], Train Loss: 0.0388, Val Loss: 0.0439\n",
      "Epoch [580/1000], Train Loss: 0.0388, Val Loss: 0.0442\n",
      "Epoch [581/1000], Train Loss: 0.0398, Val Loss: 0.0441\n",
      "Epoch [582/1000], Train Loss: 0.0397, Val Loss: 0.0440\n",
      "Epoch [583/1000], Train Loss: 0.0392, Val Loss: 0.0441\n",
      "Epoch [584/1000], Train Loss: 0.0389, Val Loss: 0.0441\n",
      "Epoch [585/1000], Train Loss: 0.0473, Val Loss: 0.0440\n",
      "Epoch [586/1000], Train Loss: 0.0397, Val Loss: 0.0440\n",
      "Epoch [587/1000], Train Loss: 0.0409, Val Loss: 0.0440\n",
      "Epoch [588/1000], Train Loss: 0.0387, Val Loss: 0.0439\n",
      "Epoch [589/1000], Train Loss: 0.0387, Val Loss: 0.0445\n",
      "Epoch [590/1000], Train Loss: 0.0390, Val Loss: 0.0438\n",
      "Best model saved with Val Loss: 0.0438\n",
      "Epoch [591/1000], Train Loss: 0.0398, Val Loss: 0.0441\n",
      "Epoch [592/1000], Train Loss: 0.0410, Val Loss: 0.0442\n",
      "Epoch [593/1000], Train Loss: 0.0390, Val Loss: 0.0439\n",
      "Epoch [594/1000], Train Loss: 0.0393, Val Loss: 0.0440\n",
      "Epoch [595/1000], Train Loss: 0.0409, Val Loss: 0.0437\n",
      "Best model saved with Val Loss: 0.0437\n",
      "Epoch [596/1000], Train Loss: 0.0389, Val Loss: 0.0437\n",
      "Epoch [597/1000], Train Loss: 0.0387, Val Loss: 0.0441\n",
      "Epoch [598/1000], Train Loss: 0.0399, Val Loss: 0.0437\n",
      "Best model saved with Val Loss: 0.0437\n",
      "Epoch [599/1000], Train Loss: 0.0434, Val Loss: 0.0438\n",
      "Epoch [600/1000], Train Loss: 0.0411, Val Loss: 0.0440\n",
      "Epoch [601/1000], Train Loss: 0.0396, Val Loss: 0.0439\n",
      "Epoch [602/1000], Train Loss: 0.0409, Val Loss: 0.0440\n",
      "Epoch [603/1000], Train Loss: 0.0386, Val Loss: 0.0437\n",
      "Best model saved with Val Loss: 0.0437\n",
      "Epoch [604/1000], Train Loss: 0.0393, Val Loss: 0.0438\n",
      "Epoch [605/1000], Train Loss: 0.0386, Val Loss: 0.0439\n",
      "Epoch [606/1000], Train Loss: 0.0389, Val Loss: 0.0442\n",
      "Epoch [607/1000], Train Loss: 0.0390, Val Loss: 0.0437\n",
      "Epoch [608/1000], Train Loss: 0.0399, Val Loss: 0.0441\n",
      "Epoch [609/1000], Train Loss: 0.0389, Val Loss: 0.0443\n",
      "Epoch [610/1000], Train Loss: 0.0418, Val Loss: 0.0437\n",
      "Epoch [611/1000], Train Loss: 0.0389, Val Loss: 0.0442\n",
      "Epoch [612/1000], Train Loss: 0.0389, Val Loss: 0.0437\n",
      "Epoch [613/1000], Train Loss: 0.0384, Val Loss: 0.0440\n",
      "Epoch [614/1000], Train Loss: 0.0411, Val Loss: 0.0439\n",
      "Epoch [615/1000], Train Loss: 0.0392, Val Loss: 0.0438\n",
      "Epoch [616/1000], Train Loss: 0.0398, Val Loss: 0.0436\n",
      "Best model saved with Val Loss: 0.0436\n",
      "Epoch [617/1000], Train Loss: 0.0417, Val Loss: 0.0440\n",
      "Epoch [618/1000], Train Loss: 0.0400, Val Loss: 0.0439\n",
      "Epoch [619/1000], Train Loss: 0.0408, Val Loss: 0.0442\n",
      "Epoch [620/1000], Train Loss: 0.0407, Val Loss: 0.0437\n",
      "Epoch [621/1000], Train Loss: 0.0391, Val Loss: 0.0437\n",
      "Epoch [622/1000], Train Loss: 0.0415, Val Loss: 0.0444\n",
      "Epoch [623/1000], Train Loss: 0.0415, Val Loss: 0.0440\n",
      "Epoch [624/1000], Train Loss: 0.0392, Val Loss: 0.0439\n",
      "Epoch [625/1000], Train Loss: 0.0386, Val Loss: 0.0440\n",
      "Epoch [626/1000], Train Loss: 0.0403, Val Loss: 0.0442\n",
      "Epoch [627/1000], Train Loss: 0.0401, Val Loss: 0.0438\n",
      "Epoch [628/1000], Train Loss: 0.0385, Val Loss: 0.0437\n",
      "Epoch [629/1000], Train Loss: 0.0389, Val Loss: 0.0439\n",
      "Epoch [630/1000], Train Loss: 0.0399, Val Loss: 0.0440\n",
      "Epoch [631/1000], Train Loss: 0.0387, Val Loss: 0.0440\n",
      "Epoch [632/1000], Train Loss: 0.0389, Val Loss: 0.0436\n",
      "Best model saved with Val Loss: 0.0436\n",
      "Epoch [633/1000], Train Loss: 0.0406, Val Loss: 0.0438\n",
      "Epoch [634/1000], Train Loss: 0.0398, Val Loss: 0.0437\n",
      "Epoch [635/1000], Train Loss: 0.0385, Val Loss: 0.0438\n",
      "Epoch [636/1000], Train Loss: 0.0399, Val Loss: 0.0439\n",
      "Epoch [637/1000], Train Loss: 0.0451, Val Loss: 0.0439\n",
      "Epoch [638/1000], Train Loss: 0.0390, Val Loss: 0.0437\n",
      "Epoch [639/1000], Train Loss: 0.0389, Val Loss: 0.0439\n",
      "Epoch [640/1000], Train Loss: 0.0386, Val Loss: 0.0436\n",
      "Best model saved with Val Loss: 0.0436\n",
      "Epoch [641/1000], Train Loss: 0.0420, Val Loss: 0.0436\n",
      "Epoch [642/1000], Train Loss: 0.0404, Val Loss: 0.0441\n",
      "Epoch [643/1000], Train Loss: 0.0409, Val Loss: 0.0436\n",
      "Best model saved with Val Loss: 0.0436\n",
      "Epoch [644/1000], Train Loss: 0.0386, Val Loss: 0.0438\n",
      "Epoch [645/1000], Train Loss: 0.0390, Val Loss: 0.0438\n",
      "Epoch [646/1000], Train Loss: 0.0398, Val Loss: 0.0443\n",
      "Epoch [647/1000], Train Loss: 0.0387, Val Loss: 0.0437\n",
      "Epoch [648/1000], Train Loss: 0.0405, Val Loss: 0.0438\n",
      "Epoch [649/1000], Train Loss: 0.0403, Val Loss: 0.0436\n",
      "Epoch [650/1000], Train Loss: 0.0387, Val Loss: 0.0437\n",
      "Epoch [651/1000], Train Loss: 0.0399, Val Loss: 0.0436\n",
      "Best model saved with Val Loss: 0.0436\n",
      "Epoch [652/1000], Train Loss: 0.0390, Val Loss: 0.0444\n",
      "Epoch [653/1000], Train Loss: 0.0387, Val Loss: 0.0436\n",
      "Epoch [654/1000], Train Loss: 0.0393, Val Loss: 0.0436\n",
      "Best model saved with Val Loss: 0.0436\n",
      "Epoch [655/1000], Train Loss: 0.0403, Val Loss: 0.0438\n",
      "Epoch [656/1000], Train Loss: 0.0392, Val Loss: 0.0435\n",
      "Best model saved with Val Loss: 0.0435\n",
      "Epoch [657/1000], Train Loss: 0.0386, Val Loss: 0.0439\n",
      "Epoch [658/1000], Train Loss: 0.0407, Val Loss: 0.0438\n",
      "Epoch [659/1000], Train Loss: 0.0386, Val Loss: 0.0440\n",
      "Epoch [660/1000], Train Loss: 0.0386, Val Loss: 0.0439\n",
      "Epoch [661/1000], Train Loss: 0.0386, Val Loss: 0.0435\n",
      "Epoch [662/1000], Train Loss: 0.0408, Val Loss: 0.0439\n",
      "Epoch [663/1000], Train Loss: 0.0408, Val Loss: 0.0437\n",
      "Epoch [664/1000], Train Loss: 0.0395, Val Loss: 0.0440\n",
      "Epoch [665/1000], Train Loss: 0.0396, Val Loss: 0.0437\n",
      "Epoch [666/1000], Train Loss: 0.0404, Val Loss: 0.0437\n",
      "Epoch [667/1000], Train Loss: 0.0391, Val Loss: 0.0435\n",
      "Epoch [668/1000], Train Loss: 0.0388, Val Loss: 0.0439\n",
      "Epoch [669/1000], Train Loss: 0.0417, Val Loss: 0.0435\n",
      "Epoch [670/1000], Train Loss: 0.0391, Val Loss: 0.0436\n",
      "Epoch [671/1000], Train Loss: 0.0386, Val Loss: 0.0435\n",
      "Epoch [672/1000], Train Loss: 0.0387, Val Loss: 0.0437\n",
      "Epoch [673/1000], Train Loss: 0.0391, Val Loss: 0.0436\n",
      "Epoch [674/1000], Train Loss: 0.0392, Val Loss: 0.0435\n",
      "Epoch [675/1000], Train Loss: 0.0390, Val Loss: 0.0435\n",
      "Epoch [676/1000], Train Loss: 0.0392, Val Loss: 0.0436\n",
      "Epoch [677/1000], Train Loss: 0.0398, Val Loss: 0.0436\n",
      "Epoch [678/1000], Train Loss: 0.0392, Val Loss: 0.0436\n",
      "Epoch [679/1000], Train Loss: 0.0420, Val Loss: 0.0434\n",
      "Best model saved with Val Loss: 0.0434\n",
      "Epoch [680/1000], Train Loss: 0.0403, Val Loss: 0.0439\n",
      "Epoch [681/1000], Train Loss: 0.0388, Val Loss: 0.0435\n",
      "Epoch [682/1000], Train Loss: 0.0390, Val Loss: 0.0440\n",
      "Epoch [683/1000], Train Loss: 0.0391, Val Loss: 0.0439\n",
      "Epoch [684/1000], Train Loss: 0.0400, Val Loss: 0.0435\n",
      "Epoch [685/1000], Train Loss: 0.0386, Val Loss: 0.0440\n",
      "Epoch [686/1000], Train Loss: 0.0393, Val Loss: 0.0435\n",
      "Epoch [687/1000], Train Loss: 0.0389, Val Loss: 0.0438\n",
      "Epoch [688/1000], Train Loss: 0.0386, Val Loss: 0.0438\n",
      "Epoch [689/1000], Train Loss: 0.0393, Val Loss: 0.0435\n",
      "Epoch [690/1000], Train Loss: 0.0392, Val Loss: 0.0436\n",
      "Epoch [691/1000], Train Loss: 0.0388, Val Loss: 0.0436\n",
      "Epoch [692/1000], Train Loss: 0.0388, Val Loss: 0.0439\n",
      "Epoch [693/1000], Train Loss: 0.0406, Val Loss: 0.0434\n",
      "Best model saved with Val Loss: 0.0434\n",
      "Epoch [694/1000], Train Loss: 0.0395, Val Loss: 0.0436\n",
      "Epoch [695/1000], Train Loss: 0.0388, Val Loss: 0.0440\n",
      "Epoch [696/1000], Train Loss: 0.0423, Val Loss: 0.0434\n",
      "Best model saved with Val Loss: 0.0434\n",
      "Epoch [697/1000], Train Loss: 0.0389, Val Loss: 0.0436\n",
      "Epoch [698/1000], Train Loss: 0.0386, Val Loss: 0.0435\n",
      "Epoch [699/1000], Train Loss: 0.0393, Val Loss: 0.0441\n",
      "Epoch [700/1000], Train Loss: 0.0401, Val Loss: 0.0434\n",
      "Epoch [701/1000], Train Loss: 0.0416, Val Loss: 0.0435\n",
      "Epoch [702/1000], Train Loss: 0.0400, Val Loss: 0.0436\n",
      "Epoch [703/1000], Train Loss: 0.0388, Val Loss: 0.0435\n",
      "Epoch [704/1000], Train Loss: 0.0384, Val Loss: 0.0438\n",
      "Epoch [705/1000], Train Loss: 0.0386, Val Loss: 0.0436\n",
      "Epoch [706/1000], Train Loss: 0.0387, Val Loss: 0.0436\n",
      "Epoch [707/1000], Train Loss: 0.0389, Val Loss: 0.0437\n",
      "Epoch [708/1000], Train Loss: 0.0385, Val Loss: 0.0433\n",
      "Best model saved with Val Loss: 0.0433\n",
      "Epoch [709/1000], Train Loss: 0.0400, Val Loss: 0.0438\n",
      "Epoch [710/1000], Train Loss: 0.0384, Val Loss: 0.0434\n",
      "Epoch [711/1000], Train Loss: 0.0388, Val Loss: 0.0436\n",
      "Epoch [712/1000], Train Loss: 0.0390, Val Loss: 0.0436\n",
      "Epoch [713/1000], Train Loss: 0.0392, Val Loss: 0.0439\n",
      "Epoch [714/1000], Train Loss: 0.0389, Val Loss: 0.0435\n",
      "Epoch [715/1000], Train Loss: 0.0392, Val Loss: 0.0434\n",
      "Epoch [716/1000], Train Loss: 0.0399, Val Loss: 0.0435\n",
      "Epoch [717/1000], Train Loss: 0.0404, Val Loss: 0.0435\n",
      "Epoch [718/1000], Train Loss: 0.0389, Val Loss: 0.0439\n",
      "Epoch [719/1000], Train Loss: 0.0408, Val Loss: 0.0436\n",
      "Epoch [720/1000], Train Loss: 0.0386, Val Loss: 0.0433\n",
      "Epoch [721/1000], Train Loss: 0.0389, Val Loss: 0.0440\n",
      "Epoch [722/1000], Train Loss: 0.0403, Val Loss: 0.0433\n",
      "Epoch [723/1000], Train Loss: 0.0384, Val Loss: 0.0442\n",
      "Epoch [724/1000], Train Loss: 0.0387, Val Loss: 0.0435\n",
      "Epoch [725/1000], Train Loss: 0.0394, Val Loss: 0.0437\n",
      "Epoch [726/1000], Train Loss: 0.0493, Val Loss: 0.0436\n",
      "Epoch [727/1000], Train Loss: 0.0392, Val Loss: 0.0438\n",
      "Epoch [728/1000], Train Loss: 0.0385, Val Loss: 0.0433\n",
      "Epoch [729/1000], Train Loss: 0.0393, Val Loss: 0.0436\n",
      "Epoch [730/1000], Train Loss: 0.0388, Val Loss: 0.0435\n",
      "Epoch [731/1000], Train Loss: 0.0388, Val Loss: 0.0434\n",
      "Epoch [732/1000], Train Loss: 0.0386, Val Loss: 0.0435\n",
      "Epoch [733/1000], Train Loss: 0.0407, Val Loss: 0.0434\n",
      "Epoch [734/1000], Train Loss: 0.0396, Val Loss: 0.0437\n",
      "Epoch [735/1000], Train Loss: 0.0395, Val Loss: 0.0433\n",
      "Epoch [736/1000], Train Loss: 0.0385, Val Loss: 0.0436\n",
      "Epoch [737/1000], Train Loss: 0.0385, Val Loss: 0.0436\n",
      "Epoch [738/1000], Train Loss: 0.0388, Val Loss: 0.0435\n",
      "Epoch [739/1000], Train Loss: 0.0396, Val Loss: 0.0435\n",
      "Epoch [740/1000], Train Loss: 0.0401, Val Loss: 0.0433\n",
      "Epoch [741/1000], Train Loss: 0.0389, Val Loss: 0.0438\n",
      "Epoch [742/1000], Train Loss: 0.0388, Val Loss: 0.0437\n",
      "Epoch [743/1000], Train Loss: 0.0392, Val Loss: 0.0438\n",
      "Epoch [744/1000], Train Loss: 0.0384, Val Loss: 0.0437\n",
      "Epoch [745/1000], Train Loss: 0.0383, Val Loss: 0.0434\n",
      "Epoch [746/1000], Train Loss: 0.0389, Val Loss: 0.0433\n",
      "Epoch [747/1000], Train Loss: 0.0382, Val Loss: 0.0434\n",
      "Epoch [748/1000], Train Loss: 0.0388, Val Loss: 0.0436\n",
      "Epoch [749/1000], Train Loss: 0.0385, Val Loss: 0.0438\n",
      "Epoch [750/1000], Train Loss: 0.0384, Val Loss: 0.0435\n",
      "Epoch [751/1000], Train Loss: 0.0383, Val Loss: 0.0434\n",
      "Epoch [752/1000], Train Loss: 0.0390, Val Loss: 0.0435\n",
      "Epoch [753/1000], Train Loss: 0.0396, Val Loss: 0.0438\n",
      "Epoch [754/1000], Train Loss: 0.0399, Val Loss: 0.0432\n",
      "Best model saved with Val Loss: 0.0432\n",
      "Epoch [755/1000], Train Loss: 0.0412, Val Loss: 0.0438\n",
      "Epoch [756/1000], Train Loss: 0.0405, Val Loss: 0.0435\n",
      "Epoch [757/1000], Train Loss: 0.0394, Val Loss: 0.0433\n",
      "Epoch [758/1000], Train Loss: 0.0400, Val Loss: 0.0436\n",
      "Epoch [759/1000], Train Loss: 0.0385, Val Loss: 0.0434\n",
      "Epoch [760/1000], Train Loss: 0.0401, Val Loss: 0.0436\n",
      "Epoch [761/1000], Train Loss: 0.0467, Val Loss: 0.0436\n",
      "Epoch [762/1000], Train Loss: 0.0386, Val Loss: 0.0435\n",
      "Epoch [763/1000], Train Loss: 0.0393, Val Loss: 0.0437\n",
      "Epoch [764/1000], Train Loss: 0.0387, Val Loss: 0.0434\n",
      "Epoch [765/1000], Train Loss: 0.0387, Val Loss: 0.0436\n",
      "Epoch [766/1000], Train Loss: 0.0382, Val Loss: 0.0433\n",
      "Epoch [767/1000], Train Loss: 0.0384, Val Loss: 0.0434\n",
      "Epoch [768/1000], Train Loss: 0.0398, Val Loss: 0.0435\n",
      "Epoch [769/1000], Train Loss: 0.0396, Val Loss: 0.0436\n",
      "Epoch [770/1000], Train Loss: 0.0389, Val Loss: 0.0432\n",
      "Epoch [771/1000], Train Loss: 0.0392, Val Loss: 0.0435\n",
      "Epoch [772/1000], Train Loss: 0.0387, Val Loss: 0.0434\n",
      "Epoch [773/1000], Train Loss: 0.0381, Val Loss: 0.0437\n",
      "Epoch [774/1000], Train Loss: 0.0386, Val Loss: 0.0435\n",
      "Epoch [775/1000], Train Loss: 0.0390, Val Loss: 0.0434\n",
      "Epoch [776/1000], Train Loss: 0.0403, Val Loss: 0.0435\n",
      "Epoch [777/1000], Train Loss: 0.0386, Val Loss: 0.0442\n",
      "Epoch [778/1000], Train Loss: 0.0428, Val Loss: 0.0434\n",
      "Epoch [779/1000], Train Loss: 0.0387, Val Loss: 0.0436\n",
      "Epoch [780/1000], Train Loss: 0.0385, Val Loss: 0.0435\n",
      "Epoch [781/1000], Train Loss: 0.0396, Val Loss: 0.0437\n",
      "Epoch [782/1000], Train Loss: 0.0385, Val Loss: 0.0432\n",
      "Epoch [783/1000], Train Loss: 0.0401, Val Loss: 0.0433\n",
      "Epoch [784/1000], Train Loss: 0.0385, Val Loss: 0.0432\n",
      "Epoch [785/1000], Train Loss: 0.0383, Val Loss: 0.0439\n",
      "Epoch [786/1000], Train Loss: 0.0411, Val Loss: 0.0436\n",
      "Epoch [787/1000], Train Loss: 0.0385, Val Loss: 0.0433\n",
      "Epoch [788/1000], Train Loss: 0.0386, Val Loss: 0.0434\n",
      "Epoch [789/1000], Train Loss: 0.0386, Val Loss: 0.0433\n",
      "Epoch [790/1000], Train Loss: 0.0382, Val Loss: 0.0434\n",
      "Epoch [791/1000], Train Loss: 0.0387, Val Loss: 0.0435\n",
      "Epoch [792/1000], Train Loss: 0.0381, Val Loss: 0.0434\n",
      "Epoch [793/1000], Train Loss: 0.0383, Val Loss: 0.0437\n",
      "Epoch [794/1000], Train Loss: 0.0386, Val Loss: 0.0436\n",
      "Epoch [795/1000], Train Loss: 0.0381, Val Loss: 0.0432\n",
      "Best model saved with Val Loss: 0.0432\n",
      "Epoch [796/1000], Train Loss: 0.0392, Val Loss: 0.0434\n",
      "Epoch [797/1000], Train Loss: 0.0499, Val Loss: 0.0432\n",
      "Best model saved with Val Loss: 0.0432\n",
      "Epoch [798/1000], Train Loss: 0.0389, Val Loss: 0.0437\n",
      "Epoch [799/1000], Train Loss: 0.0387, Val Loss: 0.0432\n",
      "Epoch [800/1000], Train Loss: 0.0390, Val Loss: 0.0435\n",
      "Epoch [801/1000], Train Loss: 0.0404, Val Loss: 0.0432\n",
      "Epoch [802/1000], Train Loss: 0.0396, Val Loss: 0.0433\n",
      "Epoch [803/1000], Train Loss: 0.0400, Val Loss: 0.0435\n",
      "Epoch [804/1000], Train Loss: 0.0408, Val Loss: 0.0437\n",
      "Epoch [805/1000], Train Loss: 0.0391, Val Loss: 0.0433\n",
      "Epoch [806/1000], Train Loss: 0.0386, Val Loss: 0.0436\n",
      "Epoch [807/1000], Train Loss: 0.0399, Val Loss: 0.0432\n",
      "Epoch [808/1000], Train Loss: 0.0388, Val Loss: 0.0437\n",
      "Epoch [809/1000], Train Loss: 0.0401, Val Loss: 0.0432\n",
      "Epoch [810/1000], Train Loss: 0.0386, Val Loss: 0.0434\n",
      "Epoch [811/1000], Train Loss: 0.0382, Val Loss: 0.0435\n",
      "Epoch [812/1000], Train Loss: 0.0388, Val Loss: 0.0434\n",
      "Epoch [813/1000], Train Loss: 0.0397, Val Loss: 0.0434\n",
      "Epoch [814/1000], Train Loss: 0.0384, Val Loss: 0.0434\n",
      "Epoch [815/1000], Train Loss: 0.0389, Val Loss: 0.0432\n",
      "Epoch [816/1000], Train Loss: 0.0388, Val Loss: 0.0432\n",
      "Epoch [817/1000], Train Loss: 0.0382, Val Loss: 0.0433\n",
      "Epoch [818/1000], Train Loss: 0.0405, Val Loss: 0.0433\n",
      "Epoch [819/1000], Train Loss: 0.0382, Val Loss: 0.0433\n",
      "Epoch [820/1000], Train Loss: 0.0383, Val Loss: 0.0442\n",
      "Epoch [821/1000], Train Loss: 0.0385, Val Loss: 0.0431\n",
      "Best model saved with Val Loss: 0.0431\n",
      "Epoch [822/1000], Train Loss: 0.0384, Val Loss: 0.0433\n",
      "Epoch [823/1000], Train Loss: 0.0398, Val Loss: 0.0436\n",
      "Epoch [824/1000], Train Loss: 0.0396, Val Loss: 0.0435\n",
      "Epoch [825/1000], Train Loss: 0.0383, Val Loss: 0.0432\n",
      "Epoch [826/1000], Train Loss: 0.0389, Val Loss: 0.0433\n",
      "Epoch [827/1000], Train Loss: 0.0385, Val Loss: 0.0431\n",
      "Epoch [828/1000], Train Loss: 0.0393, Val Loss: 0.0439\n",
      "Epoch [829/1000], Train Loss: 0.0421, Val Loss: 0.0431\n",
      "Best model saved with Val Loss: 0.0431\n",
      "Epoch [830/1000], Train Loss: 0.0390, Val Loss: 0.0432\n",
      "Epoch [831/1000], Train Loss: 0.0385, Val Loss: 0.0432\n",
      "Epoch [832/1000], Train Loss: 0.0395, Val Loss: 0.0433\n",
      "Epoch [833/1000], Train Loss: 0.0388, Val Loss: 0.0431\n",
      "Epoch [834/1000], Train Loss: 0.0385, Val Loss: 0.0433\n",
      "Epoch [835/1000], Train Loss: 0.0399, Val Loss: 0.0433\n",
      "Epoch [836/1000], Train Loss: 0.0394, Val Loss: 0.0434\n",
      "Epoch [837/1000], Train Loss: 0.0383, Val Loss: 0.0433\n",
      "Epoch [838/1000], Train Loss: 0.0383, Val Loss: 0.0434\n",
      "Epoch [839/1000], Train Loss: 0.0399, Val Loss: 0.0431\n",
      "Epoch [840/1000], Train Loss: 0.0388, Val Loss: 0.0435\n",
      "Epoch [841/1000], Train Loss: 0.0390, Val Loss: 0.0433\n",
      "Epoch [842/1000], Train Loss: 0.0396, Val Loss: 0.0433\n",
      "Epoch [843/1000], Train Loss: 0.0389, Val Loss: 0.0434\n",
      "Epoch [844/1000], Train Loss: 0.0383, Val Loss: 0.0431\n",
      "Epoch [845/1000], Train Loss: 0.0380, Val Loss: 0.0433\n",
      "Epoch [846/1000], Train Loss: 0.0385, Val Loss: 0.0432\n",
      "Epoch [847/1000], Train Loss: 0.0399, Val Loss: 0.0432\n",
      "Epoch [848/1000], Train Loss: 0.0382, Val Loss: 0.0436\n",
      "Epoch [849/1000], Train Loss: 0.0387, Val Loss: 0.0435\n",
      "Epoch [850/1000], Train Loss: 0.0400, Val Loss: 0.0433\n",
      "Epoch [851/1000], Train Loss: 0.0383, Val Loss: 0.0431\n",
      "Epoch [852/1000], Train Loss: 0.0386, Val Loss: 0.0430\n",
      "Best model saved with Val Loss: 0.0430\n",
      "Epoch [853/1000], Train Loss: 0.0388, Val Loss: 0.0434\n",
      "Epoch [854/1000], Train Loss: 0.0384, Val Loss: 0.0434\n",
      "Epoch [855/1000], Train Loss: 0.0398, Val Loss: 0.0431\n",
      "Epoch [856/1000], Train Loss: 0.0386, Val Loss: 0.0433\n",
      "Epoch [857/1000], Train Loss: 0.0392, Val Loss: 0.0432\n",
      "Epoch [858/1000], Train Loss: 0.0383, Val Loss: 0.0431\n",
      "Epoch [859/1000], Train Loss: 0.0386, Val Loss: 0.0433\n",
      "Epoch [860/1000], Train Loss: 0.0407, Val Loss: 0.0433\n",
      "Epoch [861/1000], Train Loss: 0.0387, Val Loss: 0.0435\n",
      "Epoch [862/1000], Train Loss: 0.0400, Val Loss: 0.0434\n",
      "Epoch [863/1000], Train Loss: 0.0391, Val Loss: 0.0431\n",
      "Epoch [864/1000], Train Loss: 0.0389, Val Loss: 0.0431\n",
      "Epoch [865/1000], Train Loss: 0.0399, Val Loss: 0.0432\n",
      "Epoch [866/1000], Train Loss: 0.0381, Val Loss: 0.0433\n",
      "Epoch [867/1000], Train Loss: 0.0399, Val Loss: 0.0431\n",
      "Epoch [868/1000], Train Loss: 0.0382, Val Loss: 0.0434\n",
      "Epoch [869/1000], Train Loss: 0.0466, Val Loss: 0.0433\n",
      "Epoch [870/1000], Train Loss: 0.0398, Val Loss: 0.0432\n",
      "Epoch [871/1000], Train Loss: 0.0388, Val Loss: 0.0431\n",
      "Epoch [872/1000], Train Loss: 0.0382, Val Loss: 0.0432\n",
      "Epoch [873/1000], Train Loss: 0.0387, Val Loss: 0.0431\n",
      "Epoch [874/1000], Train Loss: 0.0385, Val Loss: 0.0432\n",
      "Epoch [875/1000], Train Loss: 0.0414, Val Loss: 0.0430\n",
      "Epoch [876/1000], Train Loss: 0.0396, Val Loss: 0.0433\n",
      "Epoch [877/1000], Train Loss: 0.0396, Val Loss: 0.0433\n",
      "Epoch [878/1000], Train Loss: 0.0392, Val Loss: 0.0435\n",
      "Epoch [879/1000], Train Loss: 0.0398, Val Loss: 0.0432\n",
      "Epoch [880/1000], Train Loss: 0.0388, Val Loss: 0.0437\n",
      "Epoch [881/1000], Train Loss: 0.0406, Val Loss: 0.0430\n",
      "Epoch [882/1000], Train Loss: 0.0386, Val Loss: 0.0433\n",
      "Epoch [883/1000], Train Loss: 0.0393, Val Loss: 0.0432\n",
      "Epoch [884/1000], Train Loss: 0.0388, Val Loss: 0.0434\n",
      "Epoch [885/1000], Train Loss: 0.0382, Val Loss: 0.0430\n",
      "Best model saved with Val Loss: 0.0430\n",
      "Epoch [886/1000], Train Loss: 0.0382, Val Loss: 0.0433\n",
      "Epoch [887/1000], Train Loss: 0.0381, Val Loss: 0.0431\n",
      "Epoch [888/1000], Train Loss: 0.0393, Val Loss: 0.0430\n",
      "Epoch [889/1000], Train Loss: 0.0395, Val Loss: 0.0430\n",
      "Epoch [890/1000], Train Loss: 0.0386, Val Loss: 0.0429\n",
      "Best model saved with Val Loss: 0.0429\n",
      "Epoch [891/1000], Train Loss: 0.0383, Val Loss: 0.0431\n",
      "Epoch [892/1000], Train Loss: 0.0390, Val Loss: 0.0431\n",
      "Epoch [893/1000], Train Loss: 0.0387, Val Loss: 0.0435\n",
      "Epoch [894/1000], Train Loss: 0.0391, Val Loss: 0.0432\n",
      "Epoch [895/1000], Train Loss: 0.0380, Val Loss: 0.0431\n",
      "Epoch [896/1000], Train Loss: 0.0399, Val Loss: 0.0431\n",
      "Epoch [897/1000], Train Loss: 0.0384, Val Loss: 0.0432\n",
      "Epoch [898/1000], Train Loss: 0.0380, Val Loss: 0.0431\n",
      "Epoch [899/1000], Train Loss: 0.0400, Val Loss: 0.0430\n",
      "Epoch [900/1000], Train Loss: 0.0386, Val Loss: 0.0429\n",
      "Best model saved with Val Loss: 0.0429\n",
      "Epoch [901/1000], Train Loss: 0.0383, Val Loss: 0.0431\n",
      "Epoch [902/1000], Train Loss: 0.0383, Val Loss: 0.0432\n",
      "Epoch [903/1000], Train Loss: 0.0393, Val Loss: 0.0430\n",
      "Epoch [904/1000], Train Loss: 0.0388, Val Loss: 0.0432\n",
      "Epoch [905/1000], Train Loss: 0.0400, Val Loss: 0.0431\n",
      "Epoch [906/1000], Train Loss: 0.0387, Val Loss: 0.0433\n",
      "Epoch [907/1000], Train Loss: 0.0382, Val Loss: 0.0430\n",
      "Epoch [908/1000], Train Loss: 0.0392, Val Loss: 0.0431\n",
      "Epoch [909/1000], Train Loss: 0.0383, Val Loss: 0.0430\n",
      "Epoch [910/1000], Train Loss: 0.0381, Val Loss: 0.0430\n",
      "Epoch [911/1000], Train Loss: 0.0380, Val Loss: 0.0431\n",
      "Epoch [912/1000], Train Loss: 0.0383, Val Loss: 0.0430\n",
      "Epoch [913/1000], Train Loss: 0.0382, Val Loss: 0.0435\n",
      "Epoch [914/1000], Train Loss: 0.0399, Val Loss: 0.0429\n",
      "Best model saved with Val Loss: 0.0429\n",
      "Epoch [915/1000], Train Loss: 0.0405, Val Loss: 0.0435\n",
      "Epoch [916/1000], Train Loss: 0.0391, Val Loss: 0.0429\n",
      "Epoch [917/1000], Train Loss: 0.0389, Val Loss: 0.0433\n",
      "Epoch [918/1000], Train Loss: 0.0387, Val Loss: 0.0429\n",
      "Epoch [919/1000], Train Loss: 0.0384, Val Loss: 0.0435\n",
      "Epoch [920/1000], Train Loss: 0.0382, Val Loss: 0.0432\n",
      "Epoch [921/1000], Train Loss: 0.0390, Val Loss: 0.0432\n",
      "Epoch [922/1000], Train Loss: 0.0380, Val Loss: 0.0432\n",
      "Epoch [923/1000], Train Loss: 0.0381, Val Loss: 0.0429\n",
      "Epoch [924/1000], Train Loss: 0.0388, Val Loss: 0.0431\n",
      "Epoch [925/1000], Train Loss: 0.0391, Val Loss: 0.0433\n",
      "Epoch [926/1000], Train Loss: 0.0401, Val Loss: 0.0429\n",
      "Epoch [927/1000], Train Loss: 0.0399, Val Loss: 0.0431\n",
      "Epoch [928/1000], Train Loss: 0.0380, Val Loss: 0.0429\n",
      "Epoch [929/1000], Train Loss: 0.0402, Val Loss: 0.0430\n",
      "Epoch [930/1000], Train Loss: 0.0382, Val Loss: 0.0431\n",
      "Epoch [931/1000], Train Loss: 0.0387, Val Loss: 0.0430\n",
      "Epoch [932/1000], Train Loss: 0.0384, Val Loss: 0.0429\n",
      "Epoch [933/1000], Train Loss: 0.0397, Val Loss: 0.0430\n",
      "Epoch [934/1000], Train Loss: 0.0389, Val Loss: 0.0431\n",
      "Epoch [935/1000], Train Loss: 0.0384, Val Loss: 0.0432\n",
      "Epoch [936/1000], Train Loss: 0.0387, Val Loss: 0.0431\n",
      "Epoch [937/1000], Train Loss: 0.0383, Val Loss: 0.0430\n",
      "Epoch [938/1000], Train Loss: 0.0391, Val Loss: 0.0430\n",
      "Epoch [939/1000], Train Loss: 0.0383, Val Loss: 0.0431\n",
      "Epoch [940/1000], Train Loss: 0.0391, Val Loss: 0.0433\n",
      "Epoch [941/1000], Train Loss: 0.0399, Val Loss: 0.0432\n",
      "Epoch [942/1000], Train Loss: 0.0379, Val Loss: 0.0431\n",
      "Epoch [943/1000], Train Loss: 0.0406, Val Loss: 0.0437\n",
      "Epoch [944/1000], Train Loss: 0.0388, Val Loss: 0.0428\n",
      "Best model saved with Val Loss: 0.0428\n",
      "Epoch [945/1000], Train Loss: 0.0403, Val Loss: 0.0434\n",
      "Epoch [946/1000], Train Loss: 0.0382, Val Loss: 0.0431\n",
      "Epoch [947/1000], Train Loss: 0.0388, Val Loss: 0.0430\n",
      "Epoch [948/1000], Train Loss: 0.0391, Val Loss: 0.0431\n",
      "Epoch [949/1000], Train Loss: 0.0391, Val Loss: 0.0429\n",
      "Epoch [950/1000], Train Loss: 0.0399, Val Loss: 0.0429\n",
      "Epoch [951/1000], Train Loss: 0.0386, Val Loss: 0.0430\n",
      "Epoch [952/1000], Train Loss: 0.0386, Val Loss: 0.0430\n",
      "Epoch [953/1000], Train Loss: 0.0396, Val Loss: 0.0428\n",
      "Best model saved with Val Loss: 0.0428\n",
      "Epoch [954/1000], Train Loss: 0.0389, Val Loss: 0.0430\n",
      "Epoch [955/1000], Train Loss: 0.0380, Val Loss: 0.0435\n",
      "Epoch [956/1000], Train Loss: 0.0400, Val Loss: 0.0431\n",
      "Epoch [957/1000], Train Loss: 0.0397, Val Loss: 0.0428\n",
      "Best model saved with Val Loss: 0.0428\n",
      "Epoch [958/1000], Train Loss: 0.0378, Val Loss: 0.0433\n",
      "Epoch [959/1000], Train Loss: 0.0382, Val Loss: 0.0429\n",
      "Epoch [960/1000], Train Loss: 0.0382, Val Loss: 0.0429\n",
      "Epoch [961/1000], Train Loss: 0.0379, Val Loss: 0.0430\n",
      "Epoch [962/1000], Train Loss: 0.0387, Val Loss: 0.0431\n",
      "Epoch [963/1000], Train Loss: 0.0394, Val Loss: 0.0428\n",
      "Epoch [964/1000], Train Loss: 0.0384, Val Loss: 0.0435\n",
      "Epoch [965/1000], Train Loss: 0.0382, Val Loss: 0.0434\n",
      "Epoch [966/1000], Train Loss: 0.0378, Val Loss: 0.0431\n",
      "Epoch [967/1000], Train Loss: 0.0386, Val Loss: 0.0432\n",
      "Epoch [968/1000], Train Loss: 0.0383, Val Loss: 0.0431\n",
      "Epoch [969/1000], Train Loss: 0.0381, Val Loss: 0.0428\n",
      "Epoch [970/1000], Train Loss: 0.0389, Val Loss: 0.0430\n",
      "Epoch [971/1000], Train Loss: 0.0385, Val Loss: 0.0431\n",
      "Epoch [972/1000], Train Loss: 0.0389, Val Loss: 0.0429\n",
      "Epoch [973/1000], Train Loss: 0.0390, Val Loss: 0.0428\n",
      "Epoch [974/1000], Train Loss: 0.0380, Val Loss: 0.0430\n",
      "Epoch [975/1000], Train Loss: 0.0406, Val Loss: 0.0431\n",
      "Epoch [976/1000], Train Loss: 0.0396, Val Loss: 0.0429\n",
      "Epoch [977/1000], Train Loss: 0.0384, Val Loss: 0.0429\n",
      "Epoch [978/1000], Train Loss: 0.0383, Val Loss: 0.0428\n",
      "Epoch [979/1000], Train Loss: 0.0390, Val Loss: 0.0431\n",
      "Epoch [980/1000], Train Loss: 0.0387, Val Loss: 0.0434\n",
      "Epoch [981/1000], Train Loss: 0.0387, Val Loss: 0.0429\n",
      "Epoch [982/1000], Train Loss: 0.0394, Val Loss: 0.0431\n",
      "Epoch [983/1000], Train Loss: 0.0391, Val Loss: 0.0430\n",
      "Epoch [984/1000], Train Loss: 0.0379, Val Loss: 0.0431\n",
      "Epoch [985/1000], Train Loss: 0.0381, Val Loss: 0.0429\n",
      "Epoch [986/1000], Train Loss: 0.0398, Val Loss: 0.0429\n",
      "Epoch [987/1000], Train Loss: 0.0384, Val Loss: 0.0428\n",
      "Epoch [988/1000], Train Loss: 0.0408, Val Loss: 0.0428\n",
      "Epoch [989/1000], Train Loss: 0.0398, Val Loss: 0.0431\n",
      "Epoch [990/1000], Train Loss: 0.0380, Val Loss: 0.0431\n",
      "Epoch [991/1000], Train Loss: 0.0384, Val Loss: 0.0427\n",
      "Best model saved with Val Loss: 0.0427\n",
      "Epoch [992/1000], Train Loss: 0.0390, Val Loss: 0.0430\n",
      "Epoch [993/1000], Train Loss: 0.0383, Val Loss: 0.0429\n",
      "Epoch [994/1000], Train Loss: 0.0398, Val Loss: 0.0433\n",
      "Epoch [995/1000], Train Loss: 0.0386, Val Loss: 0.0429\n",
      "Epoch [996/1000], Train Loss: 0.0387, Val Loss: 0.0430\n",
      "Epoch [997/1000], Train Loss: 0.0394, Val Loss: 0.0428\n",
      "Epoch [998/1000], Train Loss: 0.0387, Val Loss: 0.0429\n",
      "Epoch [999/1000], Train Loss: 0.0476, Val Loss: 0.0431\n",
      "Epoch [1000/1000], Train Loss: 0.0401, Val Loss: 0.0430\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=1000, save_path='best_model_macro_npk.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c7bf7b33-21ae-4179-be4b-63258723a5ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "    # Plotting the loss graph\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, epochs+1), train_losses, label='Training Loss', color='blue')\n",
    "    plt.plot(range(1, epochs+1), val_losses, label='Validation Loss', color='red')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6619727-14c2-4ecd-a5d4-c0848b70cda4",
   "metadata": {},
   "source": [
    "EVALUATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e7bf389-9b21-41f4-b9ad-547d9daf2a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = val_df[['band1','band2','band3', 'band4', 'ndvi']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2acdaf8f-4db5-46c8-a3f7-d056dc009a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model(torch.tensor(test_data, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fa493fed-b43c-4545-94bf-540cbd16025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = predicted.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a09ab82f-f538-44ca-860d-619075b15a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N_pred</th>\n",
       "      <th>P_Pred</th>\n",
       "      <th>K_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.681773</td>\n",
       "      <td>0.169148</td>\n",
       "      <td>0.710365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.543730</td>\n",
       "      <td>0.152549</td>\n",
       "      <td>0.600656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.538013</td>\n",
       "      <td>0.150484</td>\n",
       "      <td>0.566099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.625219</td>\n",
       "      <td>0.162701</td>\n",
       "      <td>0.649228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.538230</td>\n",
       "      <td>0.153070</td>\n",
       "      <td>0.587712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>2.538183</td>\n",
       "      <td>0.159496</td>\n",
       "      <td>1.029733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>2.531534</td>\n",
       "      <td>0.159903</td>\n",
       "      <td>1.030566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>2.514443</td>\n",
       "      <td>0.159359</td>\n",
       "      <td>1.046363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>2.521100</td>\n",
       "      <td>0.159108</td>\n",
       "      <td>1.056813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>2.492871</td>\n",
       "      <td>0.157823</td>\n",
       "      <td>1.062821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>213 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       N_pred    P_Pred    K_pred\n",
       "0    2.681773  0.169148  0.710365\n",
       "1    2.543730  0.152549  0.600656\n",
       "2    2.538013  0.150484  0.566099\n",
       "3    2.625219  0.162701  0.649228\n",
       "4    2.538230  0.153070  0.587712\n",
       "..        ...       ...       ...\n",
       "208  2.538183  0.159496  1.029733\n",
       "209  2.531534  0.159903  1.030566\n",
       "210  2.514443  0.159359  1.046363\n",
       "211  2.521100  0.159108  1.056813\n",
       "212  2.492871  0.157823  1.062821\n",
       "\n",
       "[213 rows x 3 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dataframe = pd.DataFrame(output,columns=['N_pred', 'P_Pred', 'K_pred'])\n",
    "out_dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d80d0159-885b-4384-b164-e342e43acca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>band1</th>\n",
       "      <th>band2</th>\n",
       "      <th>band3</th>\n",
       "      <th>band4</th>\n",
       "      <th>ndvi</th>\n",
       "      <th>N_Actual</th>\n",
       "      <th>P_Actual</th>\n",
       "      <th>K_Actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.873684</td>\n",
       "      <td>0.549114</td>\n",
       "      <td>0.266932</td>\n",
       "      <td>0.656608</td>\n",
       "      <td>0.794398</td>\n",
       "      <td>3.03</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.879699</td>\n",
       "      <td>0.595813</td>\n",
       "      <td>0.360558</td>\n",
       "      <td>0.565178</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>2.95</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>0.866165</td>\n",
       "      <td>0.645733</td>\n",
       "      <td>0.354582</td>\n",
       "      <td>0.548280</td>\n",
       "      <td>0.740361</td>\n",
       "      <td>2.74</td>\n",
       "      <td>0.181</td>\n",
       "      <td>1.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.290837</td>\n",
       "      <td>0.593543</td>\n",
       "      <td>0.772496</td>\n",
       "      <td>2.90</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>0.851128</td>\n",
       "      <td>0.576490</td>\n",
       "      <td>0.308765</td>\n",
       "      <td>0.496681</td>\n",
       "      <td>0.738701</td>\n",
       "      <td>2.77</td>\n",
       "      <td>0.189</td>\n",
       "      <td>1.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>692</td>\n",
       "      <td>0.231579</td>\n",
       "      <td>0.388084</td>\n",
       "      <td>0.201195</td>\n",
       "      <td>0.898914</td>\n",
       "      <td>0.869902</td>\n",
       "      <td>2.71</td>\n",
       "      <td>0.160</td>\n",
       "      <td>1.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>698</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.365539</td>\n",
       "      <td>0.179283</td>\n",
       "      <td>0.857574</td>\n",
       "      <td>0.872146</td>\n",
       "      <td>2.57</td>\n",
       "      <td>0.160</td>\n",
       "      <td>1.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>700</td>\n",
       "      <td>0.193985</td>\n",
       "      <td>0.334944</td>\n",
       "      <td>0.175299</td>\n",
       "      <td>0.816838</td>\n",
       "      <td>0.873078</td>\n",
       "      <td>2.85</td>\n",
       "      <td>0.180</td>\n",
       "      <td>1.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>703</td>\n",
       "      <td>0.213534</td>\n",
       "      <td>0.351047</td>\n",
       "      <td>0.199203</td>\n",
       "      <td>0.890163</td>\n",
       "      <td>0.874772</td>\n",
       "      <td>2.93</td>\n",
       "      <td>0.180</td>\n",
       "      <td>1.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>707</td>\n",
       "      <td>0.221053</td>\n",
       "      <td>0.336554</td>\n",
       "      <td>0.193227</td>\n",
       "      <td>0.805975</td>\n",
       "      <td>0.862321</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.160</td>\n",
       "      <td>1.21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>213 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index     band1     band2     band3     band4      ndvi  N_Actual  \\\n",
       "0        1  0.873684  0.549114  0.266932  0.656608  0.794398      3.03   \n",
       "1        4  0.879699  0.595813  0.360558  0.565178  0.743590      2.95   \n",
       "2       13  0.866165  0.645733  0.354582  0.548280  0.740361      2.74   \n",
       "3       14  0.885714  0.565217  0.290837  0.593543  0.772496      2.90   \n",
       "4       20  0.851128  0.576490  0.308765  0.496681  0.738701      2.77   \n",
       "..     ...       ...       ...       ...       ...       ...       ...   \n",
       "208    692  0.231579  0.388084  0.201195  0.898914  0.869902      2.71   \n",
       "209    698  0.210526  0.365539  0.179283  0.857574  0.872146      2.57   \n",
       "210    700  0.193985  0.334944  0.175299  0.816838  0.873078      2.85   \n",
       "211    703  0.213534  0.351047  0.199203  0.890163  0.874772      2.93   \n",
       "212    707  0.221053  0.336554  0.193227  0.805975  0.862321      2.40   \n",
       "\n",
       "     P_Actual  K_Actual  \n",
       "0       0.187      0.51  \n",
       "1       0.187      0.58  \n",
       "2       0.181      1.13  \n",
       "3       0.160      0.64  \n",
       "4       0.189      1.09  \n",
       "..        ...       ...  \n",
       "208     0.160      1.12  \n",
       "209     0.160      1.24  \n",
       "210     0.180      1.17  \n",
       "211     0.180      1.14  \n",
       "212     0.160      1.21  \n",
       "\n",
       "[213 rows x 9 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataframe = val_df.reset_index()\n",
    "valid_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b9d66f39-e78f-43f3-ac08-08d479bef90b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>band1</th>\n",
       "      <th>band2</th>\n",
       "      <th>band3</th>\n",
       "      <th>band4</th>\n",
       "      <th>ndvi</th>\n",
       "      <th>N_Actual</th>\n",
       "      <th>P_Actual</th>\n",
       "      <th>K_Actual</th>\n",
       "      <th>N_pred</th>\n",
       "      <th>P_Pred</th>\n",
       "      <th>K_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.873684</td>\n",
       "      <td>0.549114</td>\n",
       "      <td>0.266932</td>\n",
       "      <td>0.656608</td>\n",
       "      <td>0.794398</td>\n",
       "      <td>3.03</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.51</td>\n",
       "      <td>2.681773</td>\n",
       "      <td>0.169148</td>\n",
       "      <td>0.710365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.879699</td>\n",
       "      <td>0.595813</td>\n",
       "      <td>0.360558</td>\n",
       "      <td>0.565178</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>2.95</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.58</td>\n",
       "      <td>2.543730</td>\n",
       "      <td>0.152549</td>\n",
       "      <td>0.600656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>0.866165</td>\n",
       "      <td>0.645733</td>\n",
       "      <td>0.354582</td>\n",
       "      <td>0.548280</td>\n",
       "      <td>0.740361</td>\n",
       "      <td>2.74</td>\n",
       "      <td>0.181</td>\n",
       "      <td>1.13</td>\n",
       "      <td>2.538013</td>\n",
       "      <td>0.150484</td>\n",
       "      <td>0.566099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.290837</td>\n",
       "      <td>0.593543</td>\n",
       "      <td>0.772496</td>\n",
       "      <td>2.90</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2.625219</td>\n",
       "      <td>0.162701</td>\n",
       "      <td>0.649228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>0.851128</td>\n",
       "      <td>0.576490</td>\n",
       "      <td>0.308765</td>\n",
       "      <td>0.496681</td>\n",
       "      <td>0.738701</td>\n",
       "      <td>2.77</td>\n",
       "      <td>0.189</td>\n",
       "      <td>1.09</td>\n",
       "      <td>2.538230</td>\n",
       "      <td>0.153070</td>\n",
       "      <td>0.587712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>692</td>\n",
       "      <td>0.231579</td>\n",
       "      <td>0.388084</td>\n",
       "      <td>0.201195</td>\n",
       "      <td>0.898914</td>\n",
       "      <td>0.869902</td>\n",
       "      <td>2.71</td>\n",
       "      <td>0.160</td>\n",
       "      <td>1.12</td>\n",
       "      <td>2.538183</td>\n",
       "      <td>0.159496</td>\n",
       "      <td>1.029733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>698</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.365539</td>\n",
       "      <td>0.179283</td>\n",
       "      <td>0.857574</td>\n",
       "      <td>0.872146</td>\n",
       "      <td>2.57</td>\n",
       "      <td>0.160</td>\n",
       "      <td>1.24</td>\n",
       "      <td>2.531534</td>\n",
       "      <td>0.159903</td>\n",
       "      <td>1.030566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>700</td>\n",
       "      <td>0.193985</td>\n",
       "      <td>0.334944</td>\n",
       "      <td>0.175299</td>\n",
       "      <td>0.816838</td>\n",
       "      <td>0.873078</td>\n",
       "      <td>2.85</td>\n",
       "      <td>0.180</td>\n",
       "      <td>1.17</td>\n",
       "      <td>2.514443</td>\n",
       "      <td>0.159359</td>\n",
       "      <td>1.046363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>703</td>\n",
       "      <td>0.213534</td>\n",
       "      <td>0.351047</td>\n",
       "      <td>0.199203</td>\n",
       "      <td>0.890163</td>\n",
       "      <td>0.874772</td>\n",
       "      <td>2.93</td>\n",
       "      <td>0.180</td>\n",
       "      <td>1.14</td>\n",
       "      <td>2.521100</td>\n",
       "      <td>0.159108</td>\n",
       "      <td>1.056813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>707</td>\n",
       "      <td>0.221053</td>\n",
       "      <td>0.336554</td>\n",
       "      <td>0.193227</td>\n",
       "      <td>0.805975</td>\n",
       "      <td>0.862321</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.160</td>\n",
       "      <td>1.21</td>\n",
       "      <td>2.492871</td>\n",
       "      <td>0.157823</td>\n",
       "      <td>1.062821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>213 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index     band1     band2     band3     band4      ndvi  N_Actual  \\\n",
       "0        1  0.873684  0.549114  0.266932  0.656608  0.794398      3.03   \n",
       "1        4  0.879699  0.595813  0.360558  0.565178  0.743590      2.95   \n",
       "2       13  0.866165  0.645733  0.354582  0.548280  0.740361      2.74   \n",
       "3       14  0.885714  0.565217  0.290837  0.593543  0.772496      2.90   \n",
       "4       20  0.851128  0.576490  0.308765  0.496681  0.738701      2.77   \n",
       "..     ...       ...       ...       ...       ...       ...       ...   \n",
       "208    692  0.231579  0.388084  0.201195  0.898914  0.869902      2.71   \n",
       "209    698  0.210526  0.365539  0.179283  0.857574  0.872146      2.57   \n",
       "210    700  0.193985  0.334944  0.175299  0.816838  0.873078      2.85   \n",
       "211    703  0.213534  0.351047  0.199203  0.890163  0.874772      2.93   \n",
       "212    707  0.221053  0.336554  0.193227  0.805975  0.862321      2.40   \n",
       "\n",
       "     P_Actual  K_Actual    N_pred    P_Pred    K_pred  \n",
       "0       0.187      0.51  2.681773  0.169148  0.710365  \n",
       "1       0.187      0.58  2.543730  0.152549  0.600656  \n",
       "2       0.181      1.13  2.538013  0.150484  0.566099  \n",
       "3       0.160      0.64  2.625219  0.162701  0.649228  \n",
       "4       0.189      1.09  2.538230  0.153070  0.587712  \n",
       "..        ...       ...       ...       ...       ...  \n",
       "208     0.160      1.12  2.538183  0.159496  1.029733  \n",
       "209     0.160      1.24  2.531534  0.159903  1.030566  \n",
       "210     0.180      1.17  2.514443  0.159359  1.046363  \n",
       "211     0.180      1.14  2.521100  0.159108  1.056813  \n",
       "212     0.160      1.21  2.492871  0.157823  1.062821  \n",
       "\n",
       "[213 rows x 12 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_dataframe = pd.concat([valid_dataframe, out_dataframe], axis=1)\n",
    "merge_dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6512c1eb-abb2-4e0f-8a96-c8caee7dfac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_dataframe.to_excel('./datasets/result_macro_npk.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1699c65b-f4c0-4fde-bb41-233037ae4e70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
